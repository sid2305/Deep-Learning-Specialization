{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network mindset\n",
    "\n",
    "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Updates</font>\n",
    "This notebook has been updated over the past few months.  The prior version was named \"v5\", and the current versionis now named '6a'\n",
    "\n",
    "#### If you were working on a previous version:\n",
    "* You can find your prior work by looking in the file directory for the older files (named by version name).\n",
    "* To view the file directory, click on the \"Coursera\" icon in the top left corner of this notebook.\n",
    "* Please copy your work from the older versions to the new version, in order to submit your work for grading.\n",
    "\n",
    "#### List of Updates\n",
    "* Forward propagation formula, indexing now starts at 1 instead of 0.\n",
    "* Optimization function comment now says \"print cost every 100 training iterations\" instead of \"examples\".\n",
    "* Fixed grammar in the comments.\n",
    "* Y_prediction_test variable name is used consistently.\n",
    "* Plot's axis label now says \"iterations (hundred)\" instead of \"iterations\".\n",
    "* When testing the model, the test image is normalized by dividing by 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f065e8f55c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "**Exercise:** Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td>**m_train**</td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**m_test**</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**num_px**</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "**Exercise:** Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>**train_set_x_flatten shape**</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**train_set_y shape**</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_x_flatten shape**</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**test_set_y shape**</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>**sanity check after reshaping**</td>\n",
    "  <td>[17 31 56 22 33]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> \n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255\n",
    "test_set_x = test_set_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What you need to remember:**\n",
    "\n",
    "Common steps for pre-processing a new dataset are:\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "**Exercise**: Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [ 0.5         0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>**sigmoid([0, 2])**</td>\n",
    "    <td> [ 0.5         0.88079708]</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initializing parameters\n",
    "\n",
    "**Exercise:** Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 0.]\n",
      " [ 0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td>  ** w **  </td>\n",
    "        <td> [[ 0.]\n",
    " [ 0.]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** b **  </td>\n",
    "        <td> 0 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For image inputs, w will be of shape (num_px $\\times$ num_px $\\times$ 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code) \n",
    "    A = sigmoid(np.dot(w.T,X)+b)                                             # compute activation\n",
    "    cost = -(np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)))/m                          # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (np.dot(X,(A-Y).T))/m\n",
    "    db = np.sum((A-Y))/m\n",
    "    ### END CODE HERE ###\n",
    "    print (dw)\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99845601]\n",
      " [ 2.39507239]]\n",
      "dw = [[ 0.99845601]\n",
      " [ 2.39507239]]\n",
      "db = 0.00145557813678\n",
      "cost = 5.80154531939\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "    <tr>\n",
    "        <td>  ** dw **  </td>\n",
    "      <td> [[ 0.99845601]\n",
    "     [ 2.39507239]]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** db **  </td>\n",
    "        <td> 0.00145557813678 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>  ** cost **  </td>\n",
    "        <td> 5.801545319394553 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99845601]\n",
      " [ 2.39507239]]\n",
      "[[ 0.99833151]\n",
      " [ 2.39467513]]\n",
      "[[ 0.99819703]\n",
      " [ 2.39424609]]\n",
      "[[ 0.9980518 ]\n",
      " [ 2.39378274]]\n",
      "[[ 0.99789497]\n",
      " [ 2.3932824 ]]\n",
      "[[ 0.99772563]\n",
      " [ 2.39274217]]\n",
      "[[ 0.9975428 ]\n",
      " [ 2.39215893]]\n",
      "[[ 0.99734543]\n",
      " [ 2.39152933]]\n",
      "[[ 0.99713238]\n",
      " [ 2.39084976]]\n",
      "[[ 0.99690246]\n",
      " [ 2.39011637]]\n",
      "[[ 0.99665435]\n",
      " [ 2.38932501]]\n",
      "[[ 0.99638666]\n",
      " [ 2.38847123]]\n",
      "[[ 0.99609788]\n",
      " [ 2.38755026]]\n",
      "[[ 0.99578643]\n",
      " [ 2.386557  ]]\n",
      "[[ 0.99545058]\n",
      " [ 2.38548599]]\n",
      "[[ 0.9950885 ]\n",
      " [ 2.38433139]]\n",
      "[[ 0.99469822]\n",
      " [ 2.38308696]]\n",
      "[[ 0.99427767]\n",
      " [ 2.38174604]]\n",
      "[[ 0.9938246 ]\n",
      " [ 2.38030155]]\n",
      "[[ 0.99333664]\n",
      " [ 2.37874594]]\n",
      "[[ 0.99281128]\n",
      " [ 2.37707116]]\n",
      "[[ 0.99224582]\n",
      " [ 2.37526869]]\n",
      "[[ 0.99163743]\n",
      " [ 2.3733295 ]]\n",
      "[[ 0.99098309]\n",
      " [ 2.37124402]]\n",
      "[[ 0.99027962]\n",
      " [ 2.36900213]]\n",
      "[[ 0.98952367]\n",
      " [ 2.36659318]]\n",
      "[[ 0.98871171]\n",
      " [ 2.36400595]]\n",
      "[[ 0.98784002]\n",
      " [ 2.36122865]]\n",
      "[[ 0.98690472]\n",
      " [ 2.35824895]]\n",
      "[[ 0.98590174]\n",
      " [ 2.35505397]]\n",
      "[[ 0.98482687]\n",
      " [ 2.3516303 ]]\n",
      "[[ 0.98367569]\n",
      " [ 2.347964  ]]\n",
      "[[ 0.98244368]\n",
      " [ 2.34404069]]\n",
      "[[ 0.98112613]\n",
      " [ 2.33984552]]\n",
      "[[ 0.97971826]\n",
      " [ 2.33536328]]\n",
      "[[ 0.97821513]\n",
      " [ 2.33057845]]\n",
      "[[ 0.97661177]\n",
      " [ 2.32547527]]\n",
      "[[ 0.97490312]\n",
      " [ 2.32003781]]\n",
      "[[ 0.97308414]\n",
      " [ 2.31425014]]\n",
      "[[ 0.97114977]\n",
      " [ 2.30809639]]\n",
      "[[ 0.96909505]\n",
      " [ 2.30156092]]\n",
      "[[ 0.96691508]\n",
      " [ 2.29462844]]\n",
      "[[ 0.96460517]\n",
      " [ 2.28728422]]\n",
      "[[ 0.9621608 ]\n",
      " [ 2.27951422]]\n",
      "[[ 0.95957775]\n",
      " [ 2.27130529]]\n",
      "[[ 0.95685212]\n",
      " [ 2.26264541]]\n",
      "[[ 0.95398041]\n",
      " [ 2.25352384]]\n",
      "[[ 0.95095957]\n",
      " [ 2.24393135]]\n",
      "[[ 0.94778709]\n",
      " [ 2.23386042]]\n",
      "[[ 0.94446102]\n",
      " [ 2.22330544]]\n",
      "[[ 0.94098008]\n",
      " [ 2.21226293]]\n",
      "[[ 0.93734368]\n",
      " [ 2.20073165]]\n",
      "[[ 0.93355195]\n",
      " [ 2.18871283]]\n",
      "[[ 0.92960584]\n",
      " [ 2.17621023]]\n",
      "[[ 0.92550709]\n",
      " [ 2.16323026]]\n",
      "[[ 0.92125828]\n",
      " [ 2.14978205]]\n",
      "[[ 0.9168628 ]\n",
      " [ 2.13587747]]\n",
      "[[ 0.91232488]\n",
      " [ 2.12153105]]\n",
      "[[ 0.90764957]\n",
      " [ 2.10675999]]\n",
      "[[ 0.90284265]\n",
      " [ 2.09158399]]\n",
      "[[ 0.89791063]\n",
      " [ 2.07602508]]\n",
      "[[ 0.89286066]\n",
      " [ 2.06010746]]\n",
      "[[ 0.88770047]\n",
      " [ 2.04385722]]\n",
      "[[ 0.88243827]\n",
      " [ 2.02730206]]\n",
      "[[ 0.87708263]\n",
      " [ 2.010471  ]]\n",
      "[[ 0.87164242]\n",
      " [ 1.99339403]]\n",
      "[[ 0.86612669]\n",
      " [ 1.97610178]]\n",
      "[[ 0.86054453]\n",
      " [ 1.95862516]]\n",
      "[[ 0.85490498]\n",
      " [ 1.94099501]]\n",
      "[[ 0.84921694]\n",
      " [ 1.92324175]]\n",
      "[[ 0.84348902]\n",
      " [ 1.90539509]]\n",
      "[[ 0.8377295 ]\n",
      " [ 1.88748367]]\n",
      "[[ 0.8319462 ]\n",
      " [ 1.86953481]]\n",
      "[[ 0.82614642]\n",
      " [ 1.85157428]]\n",
      "[[ 0.82033686]\n",
      " [ 1.83362604]]\n",
      "[[ 0.8145236 ]\n",
      " [ 1.81571212]]\n",
      "[[ 0.80871199]\n",
      " [ 1.79785242]]\n",
      "[[ 0.80290669]\n",
      " [ 1.78006466]]\n",
      "[[ 0.7971116 ]\n",
      " [ 1.76236428]]\n",
      "[[ 0.79132986]\n",
      " [ 1.7447644 ]]\n",
      "[[ 0.78556387]\n",
      " [ 1.72727583]]\n",
      "[[ 0.77981525]\n",
      " [ 1.70990709]]\n",
      "[[ 0.77408492]\n",
      " [ 1.69266446]]\n",
      "[[ 0.76837306]\n",
      " [ 1.67555205]]\n",
      "[[ 0.76267919]\n",
      " [ 1.65857191]]\n",
      "[[ 0.75700219]\n",
      " [ 1.64172409]]\n",
      "[[ 0.75134031]\n",
      " [ 1.6250068 ]]\n",
      "[[ 0.74569126]\n",
      " [ 1.60841653]]\n",
      "[[ 0.74005223]\n",
      " [ 1.59194821]]\n",
      "[[ 0.73441996]\n",
      " [ 1.57559531]]\n",
      "[[ 0.72879074]\n",
      " [ 1.55935002]]\n",
      "[[ 0.72316054]\n",
      " [ 1.54320339]]\n",
      "[[ 0.71752498]\n",
      " [ 1.52714551]]\n",
      "[[ 0.71187947]\n",
      " [ 1.51116561]]\n",
      "[[ 0.70621918]\n",
      " [ 1.49525226]]\n",
      "[[ 0.70053915]\n",
      " [ 1.47939351]]\n",
      "[[ 0.69483434]\n",
      " [ 1.46357701]]\n",
      "[[ 0.68909965]\n",
      " [ 1.44779019]]\n",
      "[[ 0.68333002]\n",
      " [ 1.43202038]]\n",
      "[[ 0.67752042]\n",
      " [ 1.41625495]]\n",
      "w = [[ 0.19033591]\n",
      " [ 0.12259159]]\n",
      "b = 1.92535983008\n",
      "dw = [[ 0.67752042]\n",
      " [ 1.41625495]]\n",
      "db = 0.219194504541\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "    <tr>\n",
    "       <td> **w** </td>\n",
    "       <td>[[ 0.19033591]\n",
    " [ 0.12259159]] </td>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "       <td> **b** </td>\n",
    "       <td> 1.92535983008 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **dw** </td>\n",
    "       <td> [[ 0.67752042]\n",
    " [ 1.41625495]] </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td> **db** </td>\n",
    "       <td> 0.219194504541 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        elif A[0,i]>0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             **predictions**\n",
    "         </td>\n",
    "          <td>\n",
    "            [[ 1.  1.  0.]]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What to remember:**\n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - computing the cost and its gradient \n",
    "    - updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "**Exercise:** Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "    \n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X, Y, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    #Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18333333]\n",
      " [ 0.11666667]]\n",
      "Cost after iteration 0: 0.693147\n",
      "[[ 0.18221729]\n",
      " [ 0.11673174]]\n",
      "[[ 0.1811075 ]\n",
      " [ 0.11679563]]\n",
      "[[ 0.18000392]\n",
      " [ 0.11685836]]\n",
      "[[ 0.17890652]\n",
      " [ 0.11691993]]\n",
      "[[ 0.17781528]\n",
      " [ 0.11698036]]\n",
      "[[ 0.17673017]\n",
      " [ 0.11703964]]\n",
      "[[ 0.17565116]\n",
      " [ 0.11709779]]\n",
      "[[ 0.17457822]\n",
      " [ 0.11715482]]\n",
      "[[ 0.17351131]\n",
      " [ 0.11721074]]\n",
      "[[ 0.17245042]\n",
      " [ 0.11726555]]\n",
      "[[ 0.1713955 ]\n",
      " [ 0.11731926]]\n",
      "[[ 0.17034654]\n",
      " [ 0.11737188]]\n",
      "[[ 0.1693035 ]\n",
      " [ 0.11742342]]\n",
      "[[ 0.16826635]\n",
      " [ 0.11747388]]\n",
      "[[ 0.16723507]\n",
      " [ 0.11752328]]\n",
      "[[ 0.16620961]\n",
      " [ 0.11757163]]\n",
      "[[ 0.16518997]\n",
      " [ 0.11761892]]\n",
      "[[ 0.16417609]\n",
      " [ 0.11766517]]\n",
      "[[ 0.16316797]\n",
      " [ 0.11771039]]\n",
      "[[ 0.16216556]\n",
      " [ 0.11775459]]\n",
      "[[ 0.16116883]\n",
      " [ 0.11779777]]\n",
      "[[ 0.16017777]\n",
      " [ 0.11783993]]\n",
      "[[ 0.15919233]\n",
      " [ 0.1178811 ]]\n",
      "[[ 0.15821249]\n",
      " [ 0.11792127]]\n",
      "[[ 0.15723823]\n",
      " [ 0.11796046]]\n",
      "[[ 0.1562695 ]\n",
      " [ 0.11799866]]\n",
      "[[ 0.15530629]\n",
      " [ 0.1180359 ]]\n",
      "[[ 0.15434856]\n",
      " [ 0.11807217]]\n",
      "[[ 0.15339628]\n",
      " [ 0.11810749]]\n",
      "[[ 0.15244943]\n",
      " [ 0.11814186]]\n",
      "[[ 0.15150798]\n",
      " [ 0.11817528]]\n",
      "[[ 0.1505719 ]\n",
      " [ 0.11820778]]\n",
      "[[ 0.14964115]\n",
      " [ 0.11823934]]\n",
      "[[ 0.14871572]\n",
      " [ 0.11826999]]\n",
      "[[ 0.14779557]\n",
      " [ 0.11829972]]\n",
      "[[ 0.14688067]\n",
      " [ 0.11832856]]\n",
      "[[ 0.145971  ]\n",
      " [ 0.11835649]]\n",
      "[[ 0.14506653]\n",
      " [ 0.11838353]]\n",
      "[[ 0.14416722]\n",
      " [ 0.11840969]]\n",
      "[[ 0.14327306]\n",
      " [ 0.11843497]]\n",
      "[[ 0.142384  ]\n",
      " [ 0.11845939]]\n",
      "[[ 0.14150004]\n",
      " [ 0.11848294]]\n",
      "[[ 0.14062113]\n",
      " [ 0.11850564]]\n",
      "[[ 0.13974724]\n",
      " [ 0.11852748]]\n",
      "[[ 0.13887836]\n",
      " [ 0.11854849]]\n",
      "[[ 0.13801446]\n",
      " [ 0.11856866]]\n",
      "[[ 0.1371555 ]\n",
      " [ 0.11858801]]\n",
      "[[ 0.13630146]\n",
      " [ 0.11860653]]\n",
      "[[ 0.1354523 ]\n",
      " [ 0.11862424]]\n",
      "[[ 0.13460802]\n",
      " [ 0.11864113]]\n",
      "[[ 0.13376857]\n",
      " [ 0.11865723]]\n",
      "[[ 0.13293392]\n",
      " [ 0.11867253]]\n",
      "[[ 0.13210407]\n",
      " [ 0.11868705]]\n",
      "[[ 0.13127896]\n",
      " [ 0.11870078]]\n",
      "[[ 0.13045859]\n",
      " [ 0.11871374]]\n",
      "[[ 0.12964291]\n",
      " [ 0.11872592]]\n",
      "[[ 0.12883192]\n",
      " [ 0.11873735]]\n",
      "[[ 0.12802557]\n",
      " [ 0.11874801]]\n",
      "[[ 0.12722384]\n",
      " [ 0.11875793]]\n",
      "[[ 0.12642671]\n",
      " [ 0.1187671 ]]\n",
      "[[ 0.12563415]\n",
      " [ 0.11877554]]\n",
      "[[ 0.12484613]\n",
      " [ 0.11878324]]\n",
      "[[ 0.12406263]\n",
      " [ 0.11879022]]\n",
      "[[ 0.12328362]\n",
      " [ 0.11879647]]\n",
      "[[ 0.12250908]\n",
      " [ 0.11880202]]\n",
      "[[ 0.12173898]\n",
      " [ 0.11880685]]\n",
      "[[ 0.12097329]\n",
      " [ 0.11881099]]\n",
      "[[ 0.120212  ]\n",
      " [ 0.11881443]]\n",
      "[[ 0.11945506]\n",
      " [ 0.11881717]]\n",
      "[[ 0.11870247]\n",
      " [ 0.11881924]]\n",
      "[[ 0.11795419]\n",
      " [ 0.11882062]]\n",
      "[[ 0.11721019]\n",
      " [ 0.11882133]]\n",
      "[[ 0.11647047]\n",
      " [ 0.11882138]]\n",
      "[[ 0.11573498]\n",
      " [ 0.11882076]]\n",
      "[[ 0.1150037 ]\n",
      " [ 0.11881949]]\n",
      "[[ 0.11427661]\n",
      " [ 0.11881757]]\n",
      "[[ 0.11355369]\n",
      " [ 0.118815  ]]\n",
      "[[ 0.11283491]\n",
      " [ 0.1188118 ]]\n",
      "[[ 0.11212025]\n",
      " [ 0.11880796]]\n",
      "[[ 0.11140968]\n",
      " [ 0.11880349]]\n",
      "[[ 0.11070318]\n",
      " [ 0.1187984 ]]\n",
      "[[ 0.11000072]\n",
      " [ 0.11879269]]\n",
      "[[ 0.10930228]\n",
      " [ 0.11878637]]\n",
      "[[ 0.10860785]\n",
      " [ 0.11877945]]\n",
      "[[ 0.10791738]\n",
      " [ 0.11877192]]\n",
      "[[ 0.10723086]\n",
      " [ 0.11876379]]\n",
      "[[ 0.10654828]\n",
      " [ 0.11875508]]\n",
      "[[ 0.10586959]\n",
      " [ 0.11874578]]\n",
      "[[ 0.10519479]\n",
      " [ 0.1187359 ]]\n",
      "[[ 0.10452384]\n",
      " [ 0.11872544]]\n",
      "[[ 0.10385673]\n",
      " [ 0.11871441]]\n",
      "[[ 0.10319344]\n",
      " [ 0.11870282]]\n",
      "[[ 0.10253393]\n",
      " [ 0.11869066]]\n",
      "[[ 0.10187819]\n",
      " [ 0.11867795]]\n",
      "[[ 0.10122619]\n",
      " [ 0.11866469]]\n",
      "[[ 0.10057792]\n",
      " [ 0.11865088]]\n",
      "[[ 0.09993335]\n",
      " [ 0.11863654]]\n",
      "[[ 0.09929245]\n",
      " [ 0.11862165]]\n",
      "[[ 0.09865521]\n",
      " [ 0.11860624]]\n",
      "[[ 0.09802161]\n",
      " [ 0.11859029]]\n",
      "Cost after iteration 100: 0.664515\n",
      "[[ 0.09739162]\n",
      " [ 0.11857383]]\n",
      "[[ 0.09676522]\n",
      " [ 0.11855685]]\n",
      "[[ 0.0961424 ]\n",
      " [ 0.11853935]]\n",
      "[[ 0.09552312]\n",
      " [ 0.11852135]]\n",
      "[[ 0.09490737]\n",
      " [ 0.11850284]]\n",
      "[[ 0.09429512]\n",
      " [ 0.11848384]]\n",
      "[[ 0.09368636]\n",
      " [ 0.11846434]]\n",
      "[[ 0.09308107]\n",
      " [ 0.11844435]]\n",
      "[[ 0.09247922]\n",
      " [ 0.11842388]]\n",
      "[[ 0.09188079]\n",
      " [ 0.11840292]]\n",
      "[[ 0.09128577]\n",
      " [ 0.11838149]]\n",
      "[[ 0.09069413]\n",
      " [ 0.11835958]]\n",
      "[[ 0.09010585]\n",
      " [ 0.11833721]]\n",
      "[[ 0.08952091]\n",
      " [ 0.11831438]]\n",
      "[[ 0.0889393 ]\n",
      " [ 0.11829108]]\n",
      "[[ 0.08836099]\n",
      " [ 0.11826733]]\n",
      "[[ 0.08778596]\n",
      " [ 0.11824313]]\n",
      "[[ 0.08721419]\n",
      " [ 0.11821848]]\n",
      "[[ 0.08664566]\n",
      " [ 0.11819339]]\n",
      "[[ 0.08608036]\n",
      " [ 0.11816786]]\n",
      "[[ 0.08551826]\n",
      " [ 0.1181419 ]]\n",
      "[[ 0.08495935]\n",
      " [ 0.1181155 ]]\n",
      "[[ 0.08440361]\n",
      " [ 0.11808868]]\n",
      "[[ 0.083851  ]\n",
      " [ 0.11806144]]\n",
      "[[ 0.08330153]\n",
      " [ 0.11803378]]\n",
      "[[ 0.08275517]\n",
      " [ 0.1180057 ]]\n",
      "[[ 0.08221189]\n",
      " [ 0.11797722]]\n",
      "[[ 0.08167169]\n",
      " [ 0.11794833]]\n",
      "[[ 0.08113454]\n",
      " [ 0.11791903]]\n",
      "[[ 0.08060043]\n",
      " [ 0.11788934]]\n",
      "[[ 0.08006933]\n",
      " [ 0.11785925]]\n",
      "[[ 0.07954123]\n",
      " [ 0.11782877]]\n",
      "[[ 0.07901611]\n",
      " [ 0.11779791]]\n",
      "[[ 0.07849395]\n",
      " [ 0.11776666]]\n",
      "[[ 0.07797473]\n",
      " [ 0.11773503]]\n",
      "[[ 0.07745844]\n",
      " [ 0.11770302]]\n",
      "[[ 0.07694507]\n",
      " [ 0.11767064]]\n",
      "[[ 0.07643458]\n",
      " [ 0.11763789]]\n",
      "[[ 0.07592696]\n",
      " [ 0.11760478]]\n",
      "[[ 0.0754222]\n",
      " [ 0.1175713]]\n",
      "[[ 0.07492029]\n",
      " [ 0.11753747]]\n",
      "[[ 0.07442119]\n",
      " [ 0.11750328]]\n",
      "[[ 0.0739249 ]\n",
      " [ 0.11746874]]\n",
      "[[ 0.07343139]\n",
      " [ 0.11743385]]\n",
      "[[ 0.07294066]\n",
      " [ 0.11739862]]\n",
      "[[ 0.07245268]\n",
      " [ 0.11736305]]\n",
      "[[ 0.07196744]\n",
      " [ 0.11732713]]\n",
      "[[ 0.07148493]\n",
      " [ 0.11729089]]\n",
      "[[ 0.07100511]\n",
      " [ 0.11725431]]\n",
      "[[ 0.07052799]\n",
      " [ 0.11721741]]\n",
      "[[ 0.07005353]\n",
      " [ 0.11718018]]\n",
      "[[ 0.06958174]\n",
      " [ 0.11714263]]\n",
      "[[ 0.06911258]\n",
      " [ 0.11710477]]\n",
      "[[ 0.06864605]\n",
      " [ 0.11706659]]\n",
      "[[ 0.06818213]\n",
      " [ 0.11702809]]\n",
      "[[ 0.0677208 ]\n",
      " [ 0.11698929]]\n",
      "[[ 0.06726204]\n",
      " [ 0.11695019]]\n",
      "[[ 0.06680585]\n",
      " [ 0.11691078]]\n",
      "[[ 0.06635221]\n",
      " [ 0.11687107]]\n",
      "[[ 0.06590109]\n",
      " [ 0.11683107]]\n",
      "[[ 0.06545249]\n",
      " [ 0.11679078]]\n",
      "[[ 0.0650064]\n",
      " [ 0.1167502]]\n",
      "[[ 0.06456278]\n",
      " [ 0.11670933]]\n",
      "[[ 0.06412164]\n",
      " [ 0.11666817]]\n",
      "[[ 0.06368295]\n",
      " [ 0.11662674]]\n",
      "[[ 0.0632467 ]\n",
      " [ 0.11658503]]\n",
      "[[ 0.06281288]\n",
      " [ 0.11654304]]\n",
      "[[ 0.06238147]\n",
      " [ 0.11650078]]\n",
      "[[ 0.06195246]\n",
      " [ 0.11645825]]\n",
      "[[ 0.06152583]\n",
      " [ 0.11641546]]\n",
      "[[ 0.06110157]\n",
      " [ 0.11637241]]\n",
      "[[ 0.06067966]\n",
      " [ 0.11632909]]\n",
      "[[ 0.06026009]\n",
      " [ 0.11628551]]\n",
      "[[ 0.05984284]\n",
      " [ 0.11624169]]\n",
      "[[ 0.05942791]\n",
      " [ 0.11619761]]\n",
      "[[ 0.05901528]\n",
      " [ 0.11615328]]\n",
      "[[ 0.05860493]\n",
      " [ 0.1161087 ]]\n",
      "[[ 0.05819685]\n",
      " [ 0.11606388]]\n",
      "[[ 0.05779102]\n",
      " [ 0.11601882]]\n",
      "[[ 0.05738744]\n",
      " [ 0.11597352]]\n",
      "[[ 0.05698609]\n",
      " [ 0.11592798]]\n",
      "[[ 0.05658695]\n",
      " [ 0.11588221]]\n",
      "[[ 0.05619002]\n",
      " [ 0.11583621]]\n",
      "[[ 0.05579527]\n",
      " [ 0.11578999]]\n",
      "[[ 0.0554027 ]\n",
      " [ 0.11574353]]\n",
      "[[ 0.0550123 ]\n",
      " [ 0.11569686]]\n",
      "[[ 0.05462404]\n",
      " [ 0.11564996]]\n",
      "[[ 0.05423793]\n",
      " [ 0.11560285]]\n",
      "[[ 0.05385393]\n",
      " [ 0.11555552]]\n",
      "[[ 0.05347205]\n",
      " [ 0.11550797]]\n",
      "[[ 0.05309227]\n",
      " [ 0.11546022]]\n",
      "[[ 0.05271458]\n",
      " [ 0.11541226]]\n",
      "[[ 0.05233896]\n",
      " [ 0.11536409]]\n",
      "[[ 0.0519654 ]\n",
      " [ 0.11531572]]\n",
      "[[ 0.05159389]\n",
      " [ 0.11526715]]\n",
      "[[ 0.05122441]\n",
      " [ 0.11521838]]\n",
      "[[ 0.05085697]\n",
      " [ 0.11516942]]\n",
      "[[ 0.05049153]\n",
      " [ 0.11512026]]\n",
      "[[ 0.0501281 ]\n",
      " [ 0.11507091]]\n",
      "[[ 0.04976665]\n",
      " [ 0.11502137]]\n",
      "Cost after iteration 200: 0.645323\n",
      "[[ 0.04940718]\n",
      " [ 0.11497164]]\n",
      "[[ 0.04904968]\n",
      " [ 0.11492173]]\n",
      "[[ 0.04869413]\n",
      " [ 0.11487164]]\n",
      "[[ 0.04834052]\n",
      " [ 0.11482136]]\n",
      "[[ 0.04798884]\n",
      " [ 0.11477091]]\n",
      "[[ 0.04763908]\n",
      " [ 0.11472028]]\n",
      "[[ 0.04729122]\n",
      " [ 0.11466948]]\n",
      "[[ 0.04694527]\n",
      " [ 0.11461851]]\n",
      "[[ 0.0466012 ]\n",
      " [ 0.11456736]]\n",
      "[[ 0.046259  ]\n",
      " [ 0.11451605]]\n",
      "[[ 0.04591866]\n",
      " [ 0.11446458]]\n",
      "[[ 0.04558017]\n",
      " [ 0.11441294]]\n",
      "[[ 0.04524353]\n",
      " [ 0.11436114]]\n",
      "[[ 0.04490871]\n",
      " [ 0.11430918]]\n",
      "[[ 0.04457571]\n",
      " [ 0.11425706]]\n",
      "[[ 0.04424452]\n",
      " [ 0.11420479]]\n",
      "[[ 0.04391512]\n",
      " [ 0.11415237]]\n",
      "[[ 0.04358751]\n",
      " [ 0.1140998 ]]\n",
      "[[ 0.04326168]\n",
      " [ 0.11404707]]\n",
      "[[ 0.04293761]\n",
      " [ 0.1139942 ]]\n",
      "[[ 0.0426153 ]\n",
      " [ 0.11394119]]\n",
      "[[ 0.04229473]\n",
      " [ 0.11388803]]\n",
      "[[ 0.04197589]\n",
      " [ 0.11383473]]\n",
      "[[ 0.04165878]\n",
      " [ 0.11378129]]\n",
      "[[ 0.04134338]\n",
      " [ 0.11372772]]\n",
      "[[ 0.04102968]\n",
      " [ 0.11367401]]\n",
      "[[ 0.04071768]\n",
      " [ 0.11362016]]\n",
      "[[ 0.04040736]\n",
      " [ 0.11356619]]\n",
      "[[ 0.04009871]\n",
      " [ 0.11351208]]\n",
      "[[ 0.03979173]\n",
      " [ 0.11345785]]\n",
      "[[ 0.03948639]\n",
      " [ 0.11340349]]\n",
      "[[ 0.03918271]\n",
      " [ 0.113349  ]]\n",
      "[[ 0.03888065]\n",
      " [ 0.11329439]]\n",
      "[[ 0.03858022]\n",
      " [ 0.11323966]]\n",
      "[[ 0.03828141]\n",
      " [ 0.11318482]]\n",
      "[[ 0.0379842 ]\n",
      " [ 0.11312985]]\n",
      "[[ 0.03768858]\n",
      " [ 0.11307477]]\n",
      "[[ 0.03739455]\n",
      " [ 0.11301957]]\n",
      "[[ 0.0371021 ]\n",
      " [ 0.11296427]]\n",
      "[[ 0.03681122]\n",
      " [ 0.11290885]]\n",
      "[[ 0.0365219 ]\n",
      " [ 0.11285332]]\n",
      "[[ 0.03623412]\n",
      " [ 0.11279769]]\n",
      "[[ 0.03594788]\n",
      " [ 0.11274195]]\n",
      "[[ 0.03566318]\n",
      " [ 0.1126861 ]]\n",
      "[[ 0.03538   ]\n",
      " [ 0.11263016]]\n",
      "[[ 0.03509833]\n",
      " [ 0.11257411]]\n",
      "[[ 0.03481816]\n",
      " [ 0.11251796]]\n",
      "[[ 0.03453949]\n",
      " [ 0.11246172]]\n",
      "[[ 0.03426231]\n",
      " [ 0.11240538]]\n",
      "[[ 0.03398661]\n",
      " [ 0.11234894]]\n",
      "[[ 0.03371237]\n",
      " [ 0.11229241]]\n",
      "[[ 0.0334396 ]\n",
      " [ 0.11223579]]\n",
      "[[ 0.03316828]\n",
      " [ 0.11217908]]\n",
      "[[ 0.0328984 ]\n",
      " [ 0.11212229]]\n",
      "[[ 0.03262996]\n",
      " [ 0.1120654 ]]\n",
      "[[ 0.03236294]\n",
      " [ 0.11200843]]\n",
      "[[ 0.03209734]\n",
      " [ 0.11195138]]\n",
      "[[ 0.03183316]\n",
      " [ 0.11189424]]\n",
      "[[ 0.03157037]\n",
      " [ 0.11183702]]\n",
      "[[ 0.03130898]\n",
      " [ 0.11177973]]\n",
      "[[ 0.03104897]\n",
      " [ 0.11172235]]\n",
      "[[ 0.03079035]\n",
      " [ 0.1116649 ]]\n",
      "[[ 0.03053309]\n",
      " [ 0.11160737]]\n",
      "[[ 0.03027719]\n",
      " [ 0.11154977]]\n",
      "[[ 0.03002265]\n",
      " [ 0.11149209]]\n",
      "[[ 0.02976946]\n",
      " [ 0.11143435]]\n",
      "[[ 0.0295176 ]\n",
      " [ 0.11137653]]\n",
      "[[ 0.02926707]\n",
      " [ 0.11131865]]\n",
      "[[ 0.02901787]\n",
      " [ 0.1112607 ]]\n",
      "[[ 0.02876998]\n",
      " [ 0.11120268]]\n",
      "[[ 0.02852339]\n",
      " [ 0.1111446 ]]\n",
      "[[ 0.02827811]\n",
      " [ 0.11108646]]\n",
      "[[ 0.02803412]\n",
      " [ 0.11102825]]\n",
      "[[ 0.02779141]\n",
      " [ 0.11096998]]\n",
      "[[ 0.02754999]\n",
      " [ 0.11091166]]\n",
      "[[ 0.02730983]\n",
      " [ 0.11085327]]\n",
      "[[ 0.02707093]\n",
      " [ 0.11079483]]\n",
      "[[ 0.02683329]\n",
      " [ 0.11073633]]\n",
      "[[ 0.0265969 ]\n",
      " [ 0.11067778]]\n",
      "[[ 0.02636175]\n",
      " [ 0.11061918]]\n",
      "[[ 0.02612783]\n",
      " [ 0.11056052]]\n",
      "[[ 0.02589514]\n",
      " [ 0.11050181]]\n",
      "[[ 0.02566367]\n",
      " [ 0.11044306]]\n",
      "[[ 0.02543341]\n",
      " [ 0.11038425]]\n",
      "[[ 0.02520436]\n",
      " [ 0.1103254 ]]\n",
      "[[ 0.0249765]\n",
      " [ 0.1102665]]\n",
      "[[ 0.02474984]\n",
      " [ 0.11020756]]\n",
      "[[ 0.02452436]\n",
      " [ 0.11014857]]\n",
      "[[ 0.02430006]\n",
      " [ 0.11008954]]\n",
      "[[ 0.02407693]\n",
      " [ 0.11003047]]\n",
      "[[ 0.02385497]\n",
      " [ 0.10997136]]\n",
      "[[ 0.02363416]\n",
      " [ 0.10991221]]\n",
      "[[ 0.0234145 ]\n",
      " [ 0.10985302]]\n",
      "[[ 0.02319599]\n",
      " [ 0.1097938 ]]\n",
      "[[ 0.02297862]\n",
      " [ 0.10973454]]\n",
      "[[ 0.02276238]\n",
      " [ 0.10967524]]\n",
      "[[ 0.02254726]\n",
      " [ 0.10961591]]\n",
      "[[ 0.02233327]\n",
      " [ 0.10955655]]\n",
      "[[ 0.02212038]\n",
      " [ 0.10949715]]\n",
      "[[ 0.0219086 ]\n",
      " [ 0.10943773]]\n",
      "Cost after iteration 300: 0.629742\n",
      "[[ 0.02169792]\n",
      " [ 0.10937827]]\n",
      "[[ 0.02148834]\n",
      " [ 0.10931879]]\n",
      "[[ 0.02127984]\n",
      " [ 0.10925928]]\n",
      "[[ 0.02107242]\n",
      " [ 0.10919974]]\n",
      "[[ 0.02086607]\n",
      " [ 0.10914018]]\n",
      "[[ 0.0206608 ]\n",
      " [ 0.10908059]]\n",
      "[[ 0.02045659]\n",
      " [ 0.10902098]]\n",
      "[[ 0.02025343]\n",
      " [ 0.10896135]]\n",
      "[[ 0.02005133]\n",
      " [ 0.10890169]]\n",
      "[[ 0.01985026]\n",
      " [ 0.10884202]]\n",
      "[[ 0.01965024]\n",
      " [ 0.10878232]]\n",
      "[[ 0.01945125]\n",
      " [ 0.10872261]]\n",
      "[[ 0.01925329]\n",
      " [ 0.10866288]]\n",
      "[[ 0.01905635]\n",
      " [ 0.10860313]]\n",
      "[[ 0.01886042]\n",
      " [ 0.10854336]]\n",
      "[[ 0.0186655 ]\n",
      " [ 0.10848358]]\n",
      "[[ 0.01847159]\n",
      " [ 0.10842379]]\n",
      "[[ 0.01827867]\n",
      " [ 0.10836398]]\n",
      "[[ 0.01808675]\n",
      " [ 0.10830416]]\n",
      "[[ 0.01789581]\n",
      " [ 0.10824433]]\n",
      "[[ 0.01770586]\n",
      " [ 0.10818449]]\n",
      "[[ 0.01751688]\n",
      " [ 0.10812463]]\n",
      "[[ 0.01732887]\n",
      " [ 0.10806477]]\n",
      "[[ 0.01714183]\n",
      " [ 0.1080049 ]]\n",
      "[[ 0.01695574]\n",
      " [ 0.10794503]]\n",
      "[[ 0.01677061]\n",
      " [ 0.10788514]]\n",
      "[[ 0.01658643]\n",
      " [ 0.10782526]]\n",
      "[[ 0.01640319]\n",
      " [ 0.10776536]]\n",
      "[[ 0.01622088]\n",
      " [ 0.10770547]]\n",
      "[[ 0.01603952]\n",
      " [ 0.10764557]]\n",
      "[[ 0.01585907]\n",
      " [ 0.10758566]]\n",
      "[[ 0.01567955]\n",
      " [ 0.10752576]]\n",
      "[[ 0.01550095]\n",
      " [ 0.10746585]]\n",
      "[[ 0.01532326]\n",
      " [ 0.10740595]]\n",
      "[[ 0.01514647]\n",
      " [ 0.10734604]]\n",
      "[[ 0.01497059]\n",
      " [ 0.10728614]]\n",
      "[[ 0.01479561]\n",
      " [ 0.10722624]]\n",
      "[[ 0.01462151]\n",
      " [ 0.10716634]]\n",
      "[[ 0.0144483 ]\n",
      " [ 0.10710645]]\n",
      "[[ 0.01427598]\n",
      " [ 0.10704656]]\n",
      "[[ 0.01410453]\n",
      " [ 0.10698668]]\n",
      "[[ 0.01393395]\n",
      " [ 0.1069268 ]]\n",
      "[[ 0.01376424]\n",
      " [ 0.10686693]]\n",
      "[[ 0.01359539]\n",
      " [ 0.10680706]]\n",
      "[[ 0.0134274 ]\n",
      " [ 0.10674721]]\n",
      "[[ 0.01326026]\n",
      " [ 0.10668736]]\n",
      "[[ 0.01309396]\n",
      " [ 0.10662753]]\n",
      "[[ 0.01292852]\n",
      " [ 0.1065677 ]]\n",
      "[[ 0.0127639 ]\n",
      " [ 0.10650788]]\n",
      "[[ 0.01260013]\n",
      " [ 0.10644808]]\n",
      "[[ 0.01243718]\n",
      " [ 0.10638829]]\n",
      "[[ 0.01227506]\n",
      " [ 0.10632851]]\n",
      "[[ 0.01211375]\n",
      " [ 0.10626874]]\n",
      "[[ 0.01195326]\n",
      " [ 0.10620899]]\n",
      "[[ 0.01179359]\n",
      " [ 0.10614925]]\n",
      "[[ 0.01163472]\n",
      " [ 0.10608953]]\n",
      "[[ 0.01147665]\n",
      " [ 0.10602983]]\n",
      "[[ 0.01131937]\n",
      " [ 0.10597014]]\n",
      "[[ 0.0111629 ]\n",
      " [ 0.10591047]]\n",
      "[[ 0.0110072 ]\n",
      " [ 0.10585082]]\n",
      "[[ 0.0108523 ]\n",
      " [ 0.10579118]]\n",
      "[[ 0.01069817]\n",
      " [ 0.10573157]]\n",
      "[[ 0.01054482]\n",
      " [ 0.10567198]]\n",
      "[[ 0.01039224]\n",
      " [ 0.1056124 ]]\n",
      "[[ 0.01024043]\n",
      " [ 0.10555285]]\n",
      "[[ 0.01008938]\n",
      " [ 0.10549332]]\n",
      "[[ 0.00993909]\n",
      " [ 0.10543381]]\n",
      "[[ 0.00978955]\n",
      " [ 0.10537432]]\n",
      "[[ 0.00964076]\n",
      " [ 0.10531486]]\n",
      "[[ 0.00949272]\n",
      " [ 0.10525542]]\n",
      "[[ 0.00934542]\n",
      " [ 0.105196  ]]\n",
      "[[ 0.00919886]\n",
      " [ 0.10513661]]\n",
      "[[ 0.00905304]\n",
      " [ 0.10507725]]\n",
      "[[ 0.00890794]\n",
      " [ 0.10501791]]\n",
      "[[ 0.00876357]\n",
      " [ 0.1049586 ]]\n",
      "[[ 0.00861992]\n",
      " [ 0.10489932]]\n",
      "[[ 0.00847699]\n",
      " [ 0.10484006]]\n",
      "[[ 0.00833477]\n",
      " [ 0.10478083]]\n",
      "[[ 0.00819326]\n",
      " [ 0.10472163]]\n",
      "[[ 0.00805246]\n",
      " [ 0.10466246]]\n",
      "[[ 0.00791236]\n",
      " [ 0.10460332]]\n",
      "[[ 0.00777296]\n",
      " [ 0.10454421]]\n",
      "[[ 0.00763426]\n",
      " [ 0.10448513]]\n",
      "[[ 0.00749624]\n",
      " [ 0.10442609]]\n",
      "[[ 0.00735892]\n",
      " [ 0.10436707]]\n",
      "[[ 0.00722227]\n",
      " [ 0.10430809]]\n",
      "[[ 0.00708631]\n",
      " [ 0.10424914]]\n",
      "[[ 0.00695102]\n",
      " [ 0.10419022]]\n",
      "[[ 0.0068164 ]\n",
      " [ 0.10413133]]\n",
      "[[ 0.00668246]\n",
      " [ 0.10407248]]\n",
      "[[ 0.00654917]\n",
      " [ 0.10401367]]\n",
      "[[ 0.00641655]\n",
      " [ 0.10395489]]\n",
      "[[ 0.00628459]\n",
      " [ 0.10389614]]\n",
      "[[ 0.00615328]\n",
      " [ 0.10383744]]\n",
      "[[ 0.00602262]\n",
      " [ 0.10377876]]\n",
      "[[ 0.00589261]\n",
      " [ 0.10372013]]\n",
      "[[ 0.00576324]\n",
      " [ 0.10366153]]\n",
      "[[ 0.00563451]\n",
      " [ 0.10360297]]\n",
      "[[ 0.00550642]\n",
      " [ 0.10354445]]\n",
      "[[ 0.00537896]\n",
      " [ 0.10348597]]\n",
      "Cost after iteration 400: 0.615814\n",
      "[[ 0.00525213]\n",
      " [ 0.10342752]]\n",
      "[[ 0.00512593]\n",
      " [ 0.10336912]]\n",
      "[[ 0.00500035]\n",
      " [ 0.10331075]]\n",
      "[[ 0.00487539]\n",
      " [ 0.10325243]]\n",
      "[[ 0.00475105]\n",
      " [ 0.10319414]]\n",
      "[[ 0.00462732]\n",
      " [ 0.1031359 ]]\n",
      "[[ 0.0045042]\n",
      " [ 0.1030777]]\n",
      "[[ 0.00438169]\n",
      " [ 0.10301954]]\n",
      "[[ 0.00425978]\n",
      " [ 0.10296142]]\n",
      "[[ 0.00413846]\n",
      " [ 0.10290334]]\n",
      "[[ 0.00401775]\n",
      " [ 0.10284531]]\n",
      "[[ 0.00389763]\n",
      " [ 0.10278732]]\n",
      "[[ 0.0037781 ]\n",
      " [ 0.10272938]]\n",
      "[[ 0.00365915]\n",
      " [ 0.10267148]]\n",
      "[[ 0.00354079]\n",
      " [ 0.10261362]]\n",
      "[[ 0.00342301]\n",
      " [ 0.10255581]]\n",
      "[[ 0.00330581]\n",
      " [ 0.10249804]]\n",
      "[[ 0.00318919]\n",
      " [ 0.10244032]]\n",
      "[[ 0.00307313]\n",
      " [ 0.10238265]]\n",
      "[[ 0.00295764]\n",
      " [ 0.10232502]]\n",
      "[[ 0.00284272]\n",
      " [ 0.10226744]]\n",
      "[[ 0.00272836]\n",
      " [ 0.1022099 ]]\n",
      "[[ 0.00261456]\n",
      " [ 0.10215241]]\n",
      "[[ 0.00250132]\n",
      " [ 0.10209497]]\n",
      "[[ 0.00238863]\n",
      " [ 0.10203758]]\n",
      "[[ 0.00227649]\n",
      " [ 0.10198023]]\n",
      "[[ 0.00216489]\n",
      " [ 0.10192294]]\n",
      "[[ 0.00205385]\n",
      " [ 0.10186569]]\n",
      "[[ 0.00194334]\n",
      " [ 0.10180849]]\n",
      "[[ 0.00183337]\n",
      " [ 0.10175135]]\n",
      "[[ 0.00172394]\n",
      " [ 0.10169425]]\n",
      "[[ 0.00161504]\n",
      " [ 0.1016372 ]]\n",
      "[[ 0.00150668]\n",
      " [ 0.1015802 ]]\n",
      "[[ 0.00139884]\n",
      " [ 0.10152325]]\n",
      "[[ 0.00129152]\n",
      " [ 0.10146636]]\n",
      "[[ 0.00118473]\n",
      " [ 0.10140951]]\n",
      "[[ 0.00107845]\n",
      " [ 0.10135272]]\n",
      "[[ 0.00097269]\n",
      " [ 0.10129597]]\n",
      "[[ 0.00086745]\n",
      " [ 0.10123928]]\n",
      "[[ 0.00076271]\n",
      " [ 0.10118265]]\n",
      "[[ 0.00065849]\n",
      " [ 0.10112606]]\n",
      "[[ 0.00055477]\n",
      " [ 0.10106953]]\n",
      "[[ 0.00045155]\n",
      " [ 0.10101305]]\n",
      "[[ 0.00034883]\n",
      " [ 0.10095663]]\n",
      "[[ 0.00024661]\n",
      " [ 0.10090025]]\n",
      "[[ 0.00014489]\n",
      " [ 0.10084394]]\n",
      "[[  4.36523317e-05]\n",
      " [  1.00787672e-01]]\n",
      "[[ -5.70908918e-05]\n",
      " [  1.00731463e-01]]\n",
      "[[-0.00015735]\n",
      " [ 0.10067531]]\n",
      "[[-0.00025712]\n",
      " [ 0.10061921]]\n",
      "[[-0.00035641]\n",
      " [ 0.10056317]]\n",
      "[[-0.00045522]\n",
      " [ 0.10050718]]\n",
      "[[-0.00055355]\n",
      " [ 0.10045124]]\n",
      "[[-0.00065141]\n",
      " [ 0.10039537]]\n",
      "[[-0.0007488 ]\n",
      " [ 0.10033955]]\n",
      "[[-0.00084571]\n",
      " [ 0.10028378]]\n",
      "[[-0.00094216]\n",
      " [ 0.10022807]]\n",
      "[[-0.00103815]\n",
      " [ 0.10017242]]\n",
      "[[-0.00113367]\n",
      " [ 0.10011683]]\n",
      "[[-0.00122874]\n",
      " [ 0.10006129]]\n",
      "[[-0.00132335]\n",
      " [ 0.10000581]]\n",
      "[[-0.0014175 ]\n",
      " [ 0.09995039]]\n",
      "[[-0.0015112 ]\n",
      " [ 0.09989503]]\n",
      "[[-0.00160445]\n",
      " [ 0.09983972]]\n",
      "[[-0.00169725]\n",
      " [ 0.09978448]]\n",
      "[[-0.0017896 ]\n",
      " [ 0.09972929]]\n",
      "[[-0.00188152]\n",
      " [ 0.09967416]]\n",
      "[[-0.00197299]\n",
      " [ 0.09961909]]\n",
      "[[-0.00206402]\n",
      " [ 0.09956407]]\n",
      "[[-0.00215462]\n",
      " [ 0.09950912]]\n",
      "[[-0.00224478]\n",
      " [ 0.09945423]]\n",
      "[[-0.00233451]\n",
      " [ 0.09939939]]\n",
      "[[-0.00242382]\n",
      " [ 0.09934462]]\n",
      "[[-0.00251269]\n",
      " [ 0.09928991]]\n",
      "[[-0.00260114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.09923525]]\n",
      "[[-0.00268917]\n",
      " [ 0.09918066]]\n",
      "[[-0.00277677]\n",
      " [ 0.09912613]]\n",
      "[[-0.00286396]\n",
      " [ 0.09907166]]\n",
      "[[-0.00295073]\n",
      " [ 0.09901725]]\n",
      "[[-0.00303709]\n",
      " [ 0.0989629 ]]\n",
      "[[-0.00312304]\n",
      " [ 0.09890861]]\n",
      "[[-0.00320857]\n",
      " [ 0.09885438]]\n",
      "[[-0.0032937 ]\n",
      " [ 0.09880022]]\n",
      "[[-0.00337842]\n",
      " [ 0.09874611]]\n",
      "[[-0.00346274]\n",
      " [ 0.09869207]]\n",
      "[[-0.00354666]\n",
      " [ 0.09863809]]\n",
      "[[-0.00363018]\n",
      " [ 0.09858418]]\n",
      "[[-0.0037133 ]\n",
      " [ 0.09853032]]\n",
      "[[-0.00379602]\n",
      " [ 0.09847653]]\n",
      "[[-0.00387835]\n",
      " [ 0.0984228 ]]\n",
      "[[-0.0039603 ]\n",
      " [ 0.09836913]]\n",
      "[[-0.00404185]\n",
      " [ 0.09831553]]\n",
      "[[-0.00412301]\n",
      " [ 0.09826198]]\n",
      "[[-0.00420379]\n",
      " [ 0.09820851]]\n",
      "[[-0.00428419]\n",
      " [ 0.09815509]]\n",
      "[[-0.0043642 ]\n",
      " [ 0.09810174]]\n",
      "[[-0.00444384]\n",
      " [ 0.09804845]]\n",
      "[[-0.0045231 ]\n",
      " [ 0.09799523]]\n",
      "[[-0.00460198]\n",
      " [ 0.09794207]]\n",
      "[[-0.00468049]\n",
      " [ 0.09788897]]\n",
      "Cost after iteration 500: 0.602815\n",
      "[[-0.00475863]\n",
      " [ 0.09783594]]\n",
      "[[-0.0048364 ]\n",
      " [ 0.09778297]]\n",
      "[[-0.0049138 ]\n",
      " [ 0.09773007]]\n",
      "[[-0.00499083]\n",
      " [ 0.09767723]]\n",
      "[[-0.0050675 ]\n",
      " [ 0.09762446]]\n",
      "[[-0.00514381]\n",
      " [ 0.09757175]]\n",
      "[[-0.00521976]\n",
      " [ 0.0975191 ]]\n",
      "[[-0.00529535]\n",
      " [ 0.09746652]]\n",
      "[[-0.00537058]\n",
      " [ 0.09741401]]\n",
      "[[-0.00544546]\n",
      " [ 0.09736156]]\n",
      "[[-0.00551998]\n",
      " [ 0.09730917]]\n",
      "[[-0.00559416]\n",
      " [ 0.09725685]]\n",
      "[[-0.00566798]\n",
      " [ 0.0972046 ]]\n",
      "[[-0.00574146]\n",
      " [ 0.09715241]]\n",
      "[[-0.00581459]\n",
      " [ 0.09710028]]\n",
      "[[-0.00588738]\n",
      " [ 0.09704823]]\n",
      "[[-0.00595982]\n",
      " [ 0.09699624]]\n",
      "[[-0.00603193]\n",
      " [ 0.09694431]]\n",
      "[[-0.00610369]\n",
      " [ 0.09689245]]\n",
      "[[-0.00617512]\n",
      " [ 0.09684066]]\n",
      "[[-0.00624621]\n",
      " [ 0.09678893]]\n",
      "[[-0.00631697]\n",
      " [ 0.09673727]]\n",
      "[[-0.00638739]\n",
      " [ 0.09668567]]\n",
      "[[-0.00645749]\n",
      " [ 0.09663414]]\n",
      "[[-0.00652726]\n",
      " [ 0.09658268]]\n",
      "[[-0.0065967 ]\n",
      " [ 0.09653128]]\n",
      "[[-0.00666581]\n",
      " [ 0.09647996]]\n",
      "[[-0.0067346 ]\n",
      " [ 0.09642869]]\n",
      "[[-0.00680307]\n",
      " [ 0.0963775 ]]\n",
      "[[-0.00687122]\n",
      " [ 0.09632637]]\n",
      "[[-0.00693905]\n",
      " [ 0.09627531]]\n",
      "[[-0.00700656]\n",
      " [ 0.09622431]]\n",
      "[[-0.00707375]\n",
      " [ 0.09617338]]\n",
      "[[-0.00714064]\n",
      " [ 0.09612252]]\n",
      "[[-0.0072072 ]\n",
      " [ 0.09607173]]\n",
      "[[-0.00727346]\n",
      " [ 0.096021  ]]\n",
      "[[-0.00733941]\n",
      " [ 0.09597034]]\n",
      "[[-0.00740505]\n",
      " [ 0.09591975]]\n",
      "[[-0.00747039]\n",
      " [ 0.09586922]]\n",
      "[[-0.00753542]\n",
      " [ 0.09581877]]\n",
      "[[-0.00760014]\n",
      " [ 0.09576838]]\n",
      "[[-0.00766457]\n",
      " [ 0.09571805]]\n",
      "[[-0.00772869]\n",
      " [ 0.0956678 ]]\n",
      "[[-0.00779252]\n",
      " [ 0.09561761]]\n",
      "[[-0.00785604]\n",
      " [ 0.09556749]]\n",
      "[[-0.00791928]\n",
      " [ 0.09551744]]\n",
      "[[-0.00798222]\n",
      " [ 0.09546746]]\n",
      "[[-0.00804486]\n",
      " [ 0.09541754]]\n",
      "[[-0.00810721]\n",
      " [ 0.09536769]]\n",
      "[[-0.00816928]\n",
      " [ 0.09531791]]\n",
      "[[-0.00823105]\n",
      " [ 0.0952682 ]]\n",
      "[[-0.00829254]\n",
      " [ 0.09521855]]\n",
      "[[-0.00835375]\n",
      " [ 0.09516898]]\n",
      "[[-0.00841466]\n",
      " [ 0.09511947]]\n",
      "[[-0.0084753 ]\n",
      " [ 0.09507003]]\n",
      "[[-0.00853566]\n",
      " [ 0.09502065]]\n",
      "[[-0.00859573]\n",
      " [ 0.09497135]]\n",
      "[[-0.00865553]\n",
      " [ 0.09492211]]\n",
      "[[-0.00871505]\n",
      " [ 0.09487295]]\n",
      "[[-0.00877429]\n",
      " [ 0.09482385]]\n",
      "[[-0.00883326]\n",
      " [ 0.09477481]]\n",
      "[[-0.00889195]\n",
      " [ 0.09472585]]\n",
      "[[-0.00895038]\n",
      " [ 0.09467696]]\n",
      "[[-0.00900853]\n",
      " [ 0.09462813]]\n",
      "[[-0.00906641]\n",
      " [ 0.09457937]]\n",
      "[[-0.00912403]\n",
      " [ 0.09453068]]\n",
      "[[-0.00918138]\n",
      " [ 0.09448206]]\n",
      "[[-0.00923847]\n",
      " [ 0.09443351]]\n",
      "[[-0.00929529]\n",
      " [ 0.09438502]]\n",
      "[[-0.00935185]\n",
      " [ 0.09433661]]\n",
      "[[-0.00940814]\n",
      " [ 0.09428826]]\n",
      "[[-0.00946418]\n",
      " [ 0.09423998]]\n",
      "[[-0.00951996]\n",
      " [ 0.09419177]]\n",
      "[[-0.00957548]\n",
      " [ 0.09414363]]\n",
      "[[-0.00963075]\n",
      " [ 0.09409555]]\n",
      "[[-0.00968576]\n",
      " [ 0.09404755]]\n",
      "[[-0.00974052]\n",
      " [ 0.09399961]]\n",
      "[[-0.00979502]\n",
      " [ 0.09395174]]\n",
      "[[-0.00984928]\n",
      " [ 0.09390394]]\n",
      "[[-0.00990328]\n",
      " [ 0.09385621]]\n",
      "[[-0.00995703]\n",
      " [ 0.09380855]]\n",
      "[[-0.01001054]\n",
      " [ 0.09376095]]\n",
      "[[-0.0100638 ]\n",
      " [ 0.09371343]]\n",
      "[[-0.01011682]\n",
      " [ 0.09366597]]\n",
      "[[-0.01016959]\n",
      " [ 0.09361858]]\n",
      "[[-0.01022212]\n",
      " [ 0.09357126]]\n",
      "[[-0.01027441]\n",
      " [ 0.09352401]]\n",
      "[[-0.01032646]\n",
      " [ 0.09347683]]\n",
      "[[-0.01037827]\n",
      " [ 0.09342971]]\n",
      "[[-0.01042984]\n",
      " [ 0.09338266]]\n",
      "[[-0.01048118]\n",
      " [ 0.09333569]]\n",
      "[[-0.01053228]\n",
      " [ 0.09328878]]\n",
      "[[-0.01058314]\n",
      " [ 0.09324194]]\n",
      "[[-0.01063377]\n",
      " [ 0.09319516]]\n",
      "[[-0.01068417]\n",
      " [ 0.09314846]]\n",
      "[[-0.01073434]\n",
      " [ 0.09310183]]\n",
      "[[-0.01078428]\n",
      " [ 0.09305526]]\n",
      "[[-0.01083399]\n",
      " [ 0.09300876]]\n",
      "[[-0.01088347]\n",
      " [ 0.09296233]]\n",
      "[[-0.01093273]\n",
      " [ 0.09291597]]\n",
      "Cost after iteration 600: 0.590445\n",
      "[[-0.01098176]\n",
      " [ 0.09286968]]\n",
      "[[-0.01103056]\n",
      " [ 0.09282345]]\n",
      "[[-0.01107915]\n",
      " [ 0.09277729]]\n",
      "[[-0.01112751]\n",
      " [ 0.09273121]]\n",
      "[[-0.01117564]\n",
      " [ 0.09268519]]\n",
      "[[-0.01122356]\n",
      " [ 0.09263924]]\n",
      "[[-0.01127126]\n",
      " [ 0.09259335]]\n",
      "[[-0.01131874]\n",
      " [ 0.09254754]]\n",
      "[[-0.01136601]\n",
      " [ 0.09250179]]\n",
      "[[-0.01141306]\n",
      " [ 0.09245611]]\n",
      "[[-0.01145989]\n",
      " [ 0.0924105 ]]\n",
      "[[-0.01150651]\n",
      " [ 0.09236496]]\n",
      "[[-0.01155292]\n",
      " [ 0.09231948]]\n",
      "[[-0.01159912]\n",
      " [ 0.09227408]]\n",
      "[[-0.0116451 ]\n",
      " [ 0.09222874]]\n",
      "[[-0.01169088]\n",
      " [ 0.09218347]]\n",
      "[[-0.01173644]\n",
      " [ 0.09213827]]\n",
      "[[-0.0117818 ]\n",
      " [ 0.09209313]]\n",
      "[[-0.01182695]\n",
      " [ 0.09204807]]\n",
      "[[-0.0118719 ]\n",
      " [ 0.09200307]]\n",
      "[[-0.01191664]\n",
      " [ 0.09195814]]\n",
      "[[-0.01196118]\n",
      " [ 0.09191328]]\n",
      "[[-0.01200552]\n",
      " [ 0.09186848]]\n",
      "[[-0.01204965]\n",
      " [ 0.09182376]]\n",
      "[[-0.01209358]\n",
      " [ 0.0917791 ]]\n",
      "[[-0.01213732]\n",
      " [ 0.09173451]]\n",
      "[[-0.01218085]\n",
      " [ 0.09168998]]\n",
      "[[-0.01222419]\n",
      " [ 0.09164553]]\n",
      "[[-0.01226732]\n",
      " [ 0.09160114]]\n",
      "[[-0.01231027]\n",
      " [ 0.09155682]]\n",
      "[[-0.01235302]\n",
      " [ 0.09151256]]\n",
      "[[-0.01239557]\n",
      " [ 0.09146838]]\n",
      "[[-0.01243793]\n",
      " [ 0.09142426]]\n",
      "[[-0.0124801 ]\n",
      " [ 0.09138021]]\n",
      "[[-0.01252207]\n",
      " [ 0.09133623]]\n",
      "[[-0.01256386]\n",
      " [ 0.09129231]]\n",
      "[[-0.01260546]\n",
      " [ 0.09124846]]\n",
      "[[-0.01264686]\n",
      " [ 0.09120468]]\n",
      "[[-0.01268808]\n",
      " [ 0.09116096]]\n",
      "[[-0.01272912]\n",
      " [ 0.09111732]]\n",
      "[[-0.01276996]\n",
      " [ 0.09107374]]\n",
      "[[-0.01281063]\n",
      " [ 0.09103022]]\n",
      "[[-0.0128511 ]\n",
      " [ 0.09098678]]\n",
      "[[-0.0128914]\n",
      " [ 0.0909434]]\n",
      "[[-0.01293151]\n",
      " [ 0.09090009]]\n",
      "[[-0.01297144]\n",
      " [ 0.09085684]]\n",
      "[[-0.01301119]\n",
      " [ 0.09081366]]\n",
      "[[-0.01305076]\n",
      " [ 0.09077055]]\n",
      "[[-0.01309015]\n",
      " [ 0.09072751]]\n",
      "[[-0.01312936]\n",
      " [ 0.09068453]]\n",
      "[[-0.01316839]\n",
      " [ 0.09064161]]\n",
      "[[-0.01320725]\n",
      " [ 0.09059877]]\n",
      "[[-0.01324593]\n",
      " [ 0.09055599]]\n",
      "[[-0.01328444]\n",
      " [ 0.09051328]]\n",
      "[[-0.01332277]\n",
      " [ 0.09047063]]\n",
      "[[-0.01336093]\n",
      " [ 0.09042805]]\n",
      "[[-0.01339892]\n",
      " [ 0.09038554]]\n",
      "[[-0.01343673]\n",
      " [ 0.09034309]]\n",
      "[[-0.01347438]\n",
      " [ 0.09030071]]\n",
      "[[-0.01351185]\n",
      " [ 0.0902584 ]]\n",
      "[[-0.01354915]\n",
      " [ 0.09021615]]\n",
      "[[-0.01358629]\n",
      " [ 0.09017396]]\n",
      "[[-0.01362326]\n",
      " [ 0.09013185]]\n",
      "[[-0.01366006]\n",
      " [ 0.0900898 ]]\n",
      "[[-0.0136967 ]\n",
      " [ 0.09004781]]\n",
      "[[-0.01373317]\n",
      " [ 0.09000589]]\n",
      "[[-0.01376947]\n",
      " [ 0.08996404]]\n",
      "[[-0.01380562]\n",
      " [ 0.08992225]]\n",
      "[[-0.01384159]\n",
      " [ 0.08988053]]\n",
      "[[-0.01387741]\n",
      " [ 0.08983887]]\n",
      "[[-0.01391307]\n",
      " [ 0.08979728]]\n",
      "[[-0.01394856]\n",
      " [ 0.08975575]]\n",
      "[[-0.0139839 ]\n",
      " [ 0.08971429]]\n",
      "[[-0.01401907]\n",
      " [ 0.0896729 ]]\n",
      "[[-0.01405409]\n",
      " [ 0.08963157]]\n",
      "[[-0.01408895]\n",
      " [ 0.0895903 ]]\n",
      "[[-0.01412365]\n",
      " [ 0.0895491 ]]\n",
      "[[-0.01415819]\n",
      " [ 0.08950797]]\n",
      "[[-0.01419258]\n",
      " [ 0.0894669 ]]\n",
      "[[-0.01422682]\n",
      " [ 0.08942589]]\n",
      "[[-0.0142609 ]\n",
      " [ 0.08938495]]\n",
      "[[-0.01429483]\n",
      " [ 0.08934408]]\n",
      "[[-0.0143286 ]\n",
      " [ 0.08930327]]\n",
      "[[-0.01436222]\n",
      " [ 0.08926252]]\n",
      "[[-0.0143957 ]\n",
      " [ 0.08922184]]\n",
      "[[-0.01442902]\n",
      " [ 0.08918122]]\n",
      "[[-0.01446219]\n",
      " [ 0.08914067]]\n",
      "[[-0.01449521]\n",
      " [ 0.08910018]]\n",
      "[[-0.01452808]\n",
      " [ 0.08905976]]\n",
      "[[-0.01456081]\n",
      " [ 0.0890194 ]]\n",
      "[[-0.01459338]\n",
      " [ 0.0889791 ]]\n",
      "[[-0.01462582]\n",
      " [ 0.08893887]]\n",
      "[[-0.0146581 ]\n",
      " [ 0.08889871]]\n",
      "[[-0.01469024]\n",
      " [ 0.0888586 ]]\n",
      "[[-0.01472224]\n",
      " [ 0.08881856]]\n",
      "[[-0.01475409]\n",
      " [ 0.08877859]]\n",
      "[[-0.0147858 ]\n",
      " [ 0.08873867]]\n",
      "[[-0.01481737]\n",
      " [ 0.08869883]]\n",
      "[[-0.01484879]\n",
      " [ 0.08865904]]\n",
      "[[-0.01488007]\n",
      " [ 0.08861932]]\n",
      "Cost after iteration 700: 0.578559\n",
      "[[-0.01491122]\n",
      " [ 0.08857966]]\n",
      "[[-0.01494222]\n",
      " [ 0.08854007]]\n",
      "[[-0.01497308]\n",
      " [ 0.08850054]]\n",
      "[[-0.01500381]\n",
      " [ 0.08846107]]\n",
      "[[-0.0150344 ]\n",
      " [ 0.08842166]]\n",
      "[[-0.01506485]\n",
      " [ 0.08838232]]\n",
      "[[-0.01509516]\n",
      " [ 0.08834304]]\n",
      "[[-0.01512534]\n",
      " [ 0.08830383]]\n",
      "[[-0.01515538]\n",
      " [ 0.08826467]]\n",
      "[[-0.01518529]\n",
      " [ 0.08822558]]\n",
      "[[-0.01521506]\n",
      " [ 0.08818656]]\n",
      "[[-0.0152447 ]\n",
      " [ 0.08814759]]\n",
      "[[-0.0152742 ]\n",
      " [ 0.08810869]]\n",
      "[[-0.01530358]\n",
      " [ 0.08806985]]\n",
      "[[-0.01533282]\n",
      " [ 0.08803107]]\n",
      "[[-0.01536193]\n",
      " [ 0.08799236]]\n",
      "[[-0.01539091]\n",
      " [ 0.0879537 ]]\n",
      "[[-0.01541976]\n",
      " [ 0.08791511]]\n",
      "[[-0.01544848]\n",
      " [ 0.08787658]]\n",
      "[[-0.01547707]\n",
      " [ 0.08783812]]\n",
      "[[-0.01550553]\n",
      " [ 0.08779971]]\n",
      "[[-0.01553387]\n",
      " [ 0.08776137]]\n",
      "[[-0.01556208]\n",
      " [ 0.08772309]]\n",
      "[[-0.01559016]\n",
      " [ 0.08768487]]\n",
      "[[-0.01561812]\n",
      " [ 0.08764671]]\n",
      "[[-0.01564595]\n",
      " [ 0.08760861]]\n",
      "[[-0.01567365]\n",
      " [ 0.08757058]]\n",
      "[[-0.01570123]\n",
      " [ 0.0875326 ]]\n",
      "[[-0.01572869]\n",
      " [ 0.08749469]]\n",
      "[[-0.01575603]\n",
      " [ 0.08745684]]\n",
      "[[-0.01578324]\n",
      " [ 0.08741905]]\n",
      "[[-0.01581033]\n",
      " [ 0.08738132]]\n",
      "[[-0.0158373 ]\n",
      " [ 0.08734365]]\n",
      "[[-0.01586415]\n",
      " [ 0.08730604]]\n",
      "[[-0.01589087]\n",
      " [ 0.0872685 ]]\n",
      "[[-0.01591748]\n",
      " [ 0.08723101]]\n",
      "[[-0.01594397]\n",
      " [ 0.08719358]]\n",
      "[[-0.01597034]\n",
      " [ 0.08715622]]\n",
      "[[-0.01599659]\n",
      " [ 0.08711892]]\n",
      "[[-0.01602273]\n",
      " [ 0.08708167]]\n",
      "[[-0.01604874]\n",
      " [ 0.08704449]]\n",
      "[[-0.01607464]\n",
      " [ 0.08700736]]\n",
      "[[-0.01610043]\n",
      " [ 0.0869703 ]]\n",
      "[[-0.01612609]\n",
      " [ 0.0869333 ]]\n",
      "[[-0.01615165]\n",
      " [ 0.08689635]]\n",
      "[[-0.01617709]\n",
      " [ 0.08685947]]\n",
      "[[-0.01620241]\n",
      " [ 0.08682265]]\n",
      "[[-0.01622762]\n",
      " [ 0.08678588]]\n",
      "[[-0.01625272]\n",
      " [ 0.08674918]]\n",
      "[[-0.01627771]\n",
      " [ 0.08671253]]\n",
      "[[-0.01630258]\n",
      " [ 0.08667595]]\n",
      "[[-0.01632734]\n",
      " [ 0.08663942]]\n",
      "[[-0.01635199]\n",
      " [ 0.08660295]]\n",
      "[[-0.01637653]\n",
      " [ 0.08656655]]\n",
      "[[-0.01640096]\n",
      " [ 0.0865302 ]]\n",
      "[[-0.01642528]\n",
      " [ 0.08649391]]\n",
      "[[-0.0164495 ]\n",
      " [ 0.08645768]]\n",
      "[[-0.0164736 ]\n",
      " [ 0.08642151]]\n",
      "[[-0.01649759]\n",
      " [ 0.08638539]]\n",
      "[[-0.01652148]\n",
      " [ 0.08634934]]\n",
      "[[-0.01654526]\n",
      " [ 0.08631334]]\n",
      "[[-0.01656894]\n",
      " [ 0.08627741]]\n",
      "[[-0.0165925 ]\n",
      " [ 0.08624153]]\n",
      "[[-0.01661597]\n",
      " [ 0.08620571]]\n",
      "[[-0.01663932]\n",
      " [ 0.08616994]]\n",
      "[[-0.01666257]\n",
      " [ 0.08613424]]\n",
      "[[-0.01668572]\n",
      " [ 0.08609859]]\n",
      "[[-0.01670877]\n",
      " [ 0.086063  ]]\n",
      "[[-0.01673171]\n",
      " [ 0.08602747]]\n",
      "[[-0.01675454]\n",
      " [ 0.085992  ]]\n",
      "[[-0.01677728]\n",
      " [ 0.08595659]]\n",
      "[[-0.01679991]\n",
      " [ 0.08592123]]\n",
      "[[-0.01682244]\n",
      " [ 0.08588593]]\n",
      "[[-0.01684487]\n",
      " [ 0.08585069]]\n",
      "[[-0.0168672]\n",
      " [ 0.0858155]]\n",
      "[[-0.01688943]\n",
      " [ 0.08578038]]\n",
      "[[-0.01691156]\n",
      " [ 0.0857453 ]]\n",
      "[[-0.01693359]\n",
      " [ 0.08571029]]\n",
      "[[-0.01695552]\n",
      " [ 0.08567534]]\n",
      "[[-0.01697735]\n",
      " [ 0.08564044]]\n",
      "[[-0.01699909]\n",
      " [ 0.08560559]]\n",
      "[[-0.01702073]\n",
      " [ 0.08557081]]\n",
      "[[-0.01704227]\n",
      " [ 0.08553608]]\n",
      "[[-0.01706371]\n",
      " [ 0.08550141]]\n",
      "[[-0.01708506]\n",
      " [ 0.08546679]]\n",
      "[[-0.01710631]\n",
      " [ 0.08543223]]\n",
      "[[-0.01712746]\n",
      " [ 0.08539773]]\n",
      "[[-0.01714852]\n",
      " [ 0.08536328]]\n",
      "[[-0.01716949]\n",
      " [ 0.08532889]]\n",
      "[[-0.01719036]\n",
      " [ 0.08529455]]\n",
      "[[-0.01721114]\n",
      " [ 0.08526027]]\n",
      "[[-0.01723182]\n",
      " [ 0.08522605]]\n",
      "[[-0.01725241]\n",
      " [ 0.08519188]]\n",
      "[[-0.01727291]\n",
      " [ 0.08515777]]\n",
      "[[-0.01729331]\n",
      " [ 0.08512371]]\n",
      "[[-0.01731363]\n",
      " [ 0.08508971]]\n",
      "[[-0.01733385]\n",
      " [ 0.08505577]]\n",
      "[[-0.01735398]\n",
      " [ 0.08502188]]\n",
      "[[-0.01737403]\n",
      " [ 0.08498804]]\n",
      "[[-0.01739398]\n",
      " [ 0.08495426]]\n",
      "Cost after iteration 800: 0.567082\n",
      "[[-0.01741384]\n",
      " [ 0.08492054]]\n",
      "[[-0.01743361]\n",
      " [ 0.08488687]]\n",
      "[[-0.01745329]\n",
      " [ 0.08485325]]\n",
      "[[-0.01747289]\n",
      " [ 0.08481969]]\n",
      "[[-0.01749239]\n",
      " [ 0.08478618]]\n",
      "[[-0.01751181]\n",
      " [ 0.08475273]]\n",
      "[[-0.01753114]\n",
      " [ 0.08471934]]\n",
      "[[-0.01755038]\n",
      " [ 0.08468599]]\n",
      "[[-0.01756954]\n",
      " [ 0.08465271]]\n",
      "[[-0.01758861]\n",
      " [ 0.08461947]]\n",
      "[[-0.01760759]\n",
      " [ 0.08458629]]\n",
      "[[-0.01762649]\n",
      " [ 0.08455317]]\n",
      "[[-0.0176453]\n",
      " [ 0.0845201]]\n",
      "[[-0.01766403]\n",
      " [ 0.08448708]]\n",
      "[[-0.01768268]\n",
      " [ 0.08445411]]\n",
      "[[-0.01770123]\n",
      " [ 0.0844212 ]]\n",
      "[[-0.01771971]\n",
      " [ 0.08438835]]\n",
      "[[-0.0177381 ]\n",
      " [ 0.08435554]]\n",
      "[[-0.01775641]\n",
      " [ 0.08432279]]\n",
      "[[-0.01777464]\n",
      " [ 0.0842901 ]]\n",
      "[[-0.01779278]\n",
      " [ 0.08425745]]\n",
      "[[-0.01781084]\n",
      " [ 0.08422486]]\n",
      "[[-0.01782882]\n",
      " [ 0.08419233]]\n",
      "[[-0.01784672]\n",
      " [ 0.08415984]]\n",
      "[[-0.01786454]\n",
      " [ 0.08412741]]\n",
      "[[-0.01788228]\n",
      " [ 0.08409503]]\n",
      "[[-0.01789993]\n",
      " [ 0.08406271]]\n",
      "[[-0.01791751]\n",
      " [ 0.08403043]]\n",
      "[[-0.01793501]\n",
      " [ 0.08399821]]\n",
      "[[-0.01795243]\n",
      " [ 0.08396604]]\n",
      "[[-0.01796977]\n",
      " [ 0.08393393]]\n",
      "[[-0.01798703]\n",
      " [ 0.08390186]]\n",
      "[[-0.01800421]\n",
      " [ 0.08386985]]\n",
      "[[-0.01802131]\n",
      " [ 0.08383789]]\n",
      "[[-0.01803834]\n",
      " [ 0.08380598]]\n",
      "[[-0.01805529]\n",
      " [ 0.08377413]]\n",
      "[[-0.01807216]\n",
      " [ 0.08374232]]\n",
      "[[-0.01808896]\n",
      " [ 0.08371057]]\n",
      "[[-0.01810568]\n",
      " [ 0.08367887]]\n",
      "[[-0.01812232]\n",
      " [ 0.08364722]]\n",
      "[[-0.01813889]\n",
      " [ 0.08361562]]\n",
      "[[-0.01815539]\n",
      " [ 0.08358407]]\n",
      "[[-0.0181718 ]\n",
      " [ 0.08355258]]\n",
      "[[-0.01818815]\n",
      " [ 0.08352113]]\n",
      "[[-0.01820442]\n",
      " [ 0.08348974]]\n",
      "[[-0.01822061]\n",
      " [ 0.0834584 ]]\n",
      "[[-0.01823673]\n",
      " [ 0.0834271 ]]\n",
      "[[-0.01825278]\n",
      " [ 0.08339586]]\n",
      "[[-0.01826875]\n",
      " [ 0.08336467]]\n",
      "[[-0.01828466]\n",
      " [ 0.08333353]]\n",
      "[[-0.01830049]\n",
      " [ 0.08330244]]\n",
      "[[-0.01831624]\n",
      " [ 0.0832714 ]]\n",
      "[[-0.01833193]\n",
      " [ 0.08324041]]\n",
      "[[-0.01834754]\n",
      " [ 0.08320948]]\n",
      "[[-0.01836309]\n",
      " [ 0.08317859]]\n",
      "[[-0.01837856]\n",
      " [ 0.08314775]]\n",
      "[[-0.01839396]\n",
      " [ 0.08311696]]\n",
      "[[-0.01840929]\n",
      " [ 0.08308622]]\n",
      "[[-0.01842455]\n",
      " [ 0.08305553]]\n",
      "[[-0.01843974]\n",
      " [ 0.08302489]]\n",
      "[[-0.01845486]\n",
      " [ 0.0829943 ]]\n",
      "[[-0.01846991]\n",
      " [ 0.08296376]]\n",
      "[[-0.0184849 ]\n",
      " [ 0.08293327]]\n",
      "[[-0.01849981]\n",
      " [ 0.08290283]]\n",
      "[[-0.01851466]\n",
      " [ 0.08287244]]\n",
      "[[-0.01852944]\n",
      " [ 0.0828421 ]]\n",
      "[[-0.01854415]\n",
      " [ 0.0828118 ]]\n",
      "[[-0.01855879]\n",
      " [ 0.08278156]]\n",
      "[[-0.01857337]\n",
      " [ 0.08275136]]\n",
      "[[-0.01858788]\n",
      " [ 0.08272121]]\n",
      "[[-0.01860232]\n",
      " [ 0.08269111]]\n",
      "[[-0.0186167 ]\n",
      " [ 0.08266106]]\n",
      "[[-0.01863101]\n",
      " [ 0.08263106]]\n",
      "[[-0.01864525]\n",
      " [ 0.08260111]]\n",
      "[[-0.01865943]\n",
      " [ 0.0825712 ]]\n",
      "[[-0.01867354]\n",
      " [ 0.08254135]]\n",
      "[[-0.01868759]\n",
      " [ 0.08251154]]\n",
      "[[-0.01870158]\n",
      " [ 0.08248178]]\n",
      "[[-0.01871549]\n",
      " [ 0.08245206]]\n",
      "[[-0.01872935]\n",
      " [ 0.0824224 ]]\n",
      "[[-0.01874314]\n",
      " [ 0.08239278]]\n",
      "[[-0.01875687]\n",
      " [ 0.08236321]]\n",
      "[[-0.01877053]\n",
      " [ 0.08233369]]\n",
      "[[-0.01878414]\n",
      " [ 0.08230422]]\n",
      "[[-0.01879767]\n",
      " [ 0.08227479]]\n",
      "[[-0.01881115]\n",
      " [ 0.08224541]]\n",
      "[[-0.01882456]\n",
      " [ 0.08221608]]\n",
      "[[-0.01883792]\n",
      " [ 0.08218679]]\n",
      "[[-0.01885121]\n",
      " [ 0.08215755]]\n",
      "[[-0.01886443]\n",
      " [ 0.08212836]]\n",
      "[[-0.0188776 ]\n",
      " [ 0.08209922]]\n",
      "[[-0.01889071]\n",
      " [ 0.08207012]]\n",
      "[[-0.01890375]\n",
      " [ 0.08204107]]\n",
      "[[-0.01891674]\n",
      " [ 0.08201206]]\n",
      "[[-0.01892966]\n",
      " [ 0.08198311]]\n",
      "[[-0.01894253]\n",
      " [ 0.08195419]]\n",
      "[[-0.01895533]\n",
      " [ 0.08192533]]\n",
      "[[-0.01896808]\n",
      " [ 0.08189651]]\n",
      "[[-0.01898076]\n",
      " [ 0.08186774]]\n",
      "[[-0.01899339]\n",
      " [ 0.08183901]]\n",
      "Cost after iteration 900: 0.555970\n",
      "[[-0.01900596]\n",
      " [ 0.08181033]]\n",
      "[[-0.01901846]\n",
      " [ 0.08178169]]\n",
      "[[-0.01903092]\n",
      " [ 0.0817531 ]]\n",
      "[[-0.01904331]\n",
      " [ 0.08172456]]\n",
      "[[-0.01905564]\n",
      " [ 0.08169606]]\n",
      "[[-0.01906792]\n",
      " [ 0.08166761]]\n",
      "[[-0.01908014]\n",
      " [ 0.0816392 ]]\n",
      "[[-0.0190923 ]\n",
      " [ 0.08161084]]\n",
      "[[-0.01910441]\n",
      " [ 0.08158253]]\n",
      "[[-0.01911646]\n",
      " [ 0.08155426]]\n",
      "[[-0.01912845]\n",
      " [ 0.08152603]]\n",
      "[[-0.01914039]\n",
      " [ 0.08149785]]\n",
      "[[-0.01915227]\n",
      " [ 0.08146971]]\n",
      "[[-0.0191641 ]\n",
      " [ 0.08144162]]\n",
      "[[-0.01917587]\n",
      " [ 0.08141358]]\n",
      "[[-0.01918758]\n",
      " [ 0.08138557]]\n",
      "[[-0.01919924]\n",
      " [ 0.08135762]]\n",
      "[[-0.01921084]\n",
      " [ 0.0813297 ]]\n",
      "[[-0.01922239]\n",
      " [ 0.08130184]]\n",
      "[[-0.01923389]\n",
      " [ 0.08127401]]\n",
      "[[-0.01924533]\n",
      " [ 0.08124623]]\n",
      "[[-0.01925672]\n",
      " [ 0.0812185 ]]\n",
      "[[-0.01926805]\n",
      " [ 0.0811908 ]]\n",
      "[[-0.01927933]\n",
      " [ 0.08116316]]\n",
      "[[-0.01929056]\n",
      " [ 0.08113555]]\n",
      "[[-0.01930173]\n",
      " [ 0.08110799]]\n",
      "[[-0.01931285]\n",
      " [ 0.08108048]]\n",
      "[[-0.01932392]\n",
      " [ 0.081053  ]]\n",
      "[[-0.01933494]\n",
      " [ 0.08102558]]\n",
      "[[-0.0193459 ]\n",
      " [ 0.08099819]]\n",
      "[[-0.01935681]\n",
      " [ 0.08097085]]\n",
      "[[-0.01936767]\n",
      " [ 0.08094355]]\n",
      "[[-0.01937848]\n",
      " [ 0.08091629]]\n",
      "[[-0.01938924]\n",
      " [ 0.08088908]]\n",
      "[[-0.01939994]\n",
      " [ 0.08086191]]\n",
      "[[-0.0194106 ]\n",
      " [ 0.08083478]]\n",
      "[[-0.0194212]\n",
      " [ 0.0808077]]\n",
      "[[-0.01943176]\n",
      " [ 0.08078066]]\n",
      "[[-0.01944226]\n",
      " [ 0.08075366]]\n",
      "[[-0.01945271]\n",
      " [ 0.0807267 ]]\n",
      "[[-0.01946311]\n",
      " [ 0.08069979]]\n",
      "[[-0.01947347]\n",
      " [ 0.08067292]]\n",
      "[[-0.01948377]\n",
      " [ 0.08064609]]\n",
      "[[-0.01949403]\n",
      " [ 0.0806193 ]]\n",
      "[[-0.01950423]\n",
      " [ 0.08059256]]\n",
      "[[-0.01951439]\n",
      " [ 0.08056586]]\n",
      "[[-0.0195245]\n",
      " [ 0.0805392]]\n",
      "[[-0.01953455]\n",
      " [ 0.08051258]]\n",
      "[[-0.01954456]\n",
      " [ 0.080486  ]]\n",
      "[[-0.01955453]\n",
      " [ 0.08045947]]\n",
      "[[-0.01956444]\n",
      " [ 0.08043297]]\n",
      "[[-0.01957431]\n",
      " [ 0.08040652]]\n",
      "[[-0.01958413]\n",
      " [ 0.08038011]]\n",
      "[[-0.0195939 ]\n",
      " [ 0.08035374]]\n",
      "[[-0.01960362]\n",
      " [ 0.08032741]]\n",
      "[[-0.0196133 ]\n",
      " [ 0.08030113]]\n",
      "[[-0.01962293]\n",
      " [ 0.08027488]]\n",
      "[[-0.01963252]\n",
      " [ 0.08024868]]\n",
      "[[-0.01964205]\n",
      " [ 0.08022251]]\n",
      "[[-0.01965155]\n",
      " [ 0.08019639]]\n",
      "[[-0.01966099]\n",
      " [ 0.08017031]]\n",
      "[[-0.01967039]\n",
      " [ 0.08014427]]\n",
      "[[-0.01967974]\n",
      " [ 0.08011827]]\n",
      "[[-0.01968905]\n",
      " [ 0.08009231]]\n",
      "[[-0.01969832]\n",
      " [ 0.08006639]]\n",
      "[[-0.01970753]\n",
      " [ 0.08004051]]\n",
      "[[-0.01971671]\n",
      " [ 0.08001467]]\n",
      "[[-0.01972583]\n",
      " [ 0.07998888]]\n",
      "[[-0.01973492]\n",
      " [ 0.07996312]]\n",
      "[[-0.01974396]\n",
      " [ 0.0799374 ]]\n",
      "[[-0.01975295]\n",
      " [ 0.07991172]]\n",
      "[[-0.0197619 ]\n",
      " [ 0.07988608]]\n",
      "[[-0.01977081]\n",
      " [ 0.07986049]]\n",
      "[[-0.01977967]\n",
      " [ 0.07983493]]\n",
      "[[-0.01978849]\n",
      " [ 0.07980941]]\n",
      "[[-0.01979726]\n",
      " [ 0.07978393]]\n",
      "[[-0.019806  ]\n",
      " [ 0.07975849]]\n",
      "[[-0.01981469]\n",
      " [ 0.07973309]]\n",
      "[[-0.01982333]\n",
      " [ 0.07970773]]\n",
      "[[-0.01983194]\n",
      " [ 0.0796824 ]]\n",
      "[[-0.0198405 ]\n",
      " [ 0.07965712]]\n",
      "[[-0.01984902]\n",
      " [ 0.07963188]]\n",
      "[[-0.01985749]\n",
      " [ 0.07960667]]\n",
      "[[-0.01986593]\n",
      " [ 0.07958151]]\n",
      "[[-0.01987432]\n",
      " [ 0.07955638]]\n",
      "[[-0.01988267]\n",
      " [ 0.07953129]]\n",
      "[[-0.01989098]\n",
      " [ 0.07950624]]\n",
      "[[-0.01989925]\n",
      " [ 0.07948123]]\n",
      "[[-0.01990748]\n",
      " [ 0.07945626]]\n",
      "[[-0.01991566]\n",
      " [ 0.07943133]]\n",
      "[[-0.01992381]\n",
      " [ 0.07940643]]\n",
      "[[-0.01993191]\n",
      " [ 0.07938157]]\n",
      "[[-0.01993997]\n",
      " [ 0.07935675]]\n",
      "[[-0.019948  ]\n",
      " [ 0.07933197]]\n",
      "[[-0.01995598]\n",
      " [ 0.07930723]]\n",
      "[[-0.01996392]\n",
      " [ 0.07928252]]\n",
      "[[-0.01997183]\n",
      " [ 0.07925786]]\n",
      "[[-0.01997969]\n",
      " [ 0.07923323]]\n",
      "[[-0.01998751]\n",
      " [ 0.07920864]]\n",
      "[[-0.0199953 ]\n",
      " [ 0.07918408]]\n",
      "Cost after iteration 1000: 0.545193\n",
      "[[-0.02000304]\n",
      " [ 0.07915957]]\n",
      "[[-0.02001075]\n",
      " [ 0.07913509]]\n",
      "[[-0.02001841]\n",
      " [ 0.07911065]]\n",
      "[[-0.02002604]\n",
      " [ 0.07908624]]\n",
      "[[-0.02003363]\n",
      " [ 0.07906187]]\n",
      "[[-0.02004118]\n",
      " [ 0.07903754]]\n",
      "[[-0.02004869]\n",
      " [ 0.07901325]]\n",
      "[[-0.02005616]\n",
      " [ 0.078989  ]]\n",
      "[[-0.0200636 ]\n",
      " [ 0.07896478]]\n",
      "[[-0.020071  ]\n",
      " [ 0.07894059]]\n",
      "[[-0.02007836]\n",
      " [ 0.07891645]]\n",
      "[[-0.02008568]\n",
      " [ 0.07889234]]\n",
      "[[-0.02009296]\n",
      " [ 0.07886827]]\n",
      "[[-0.02010021]\n",
      " [ 0.07884423]]\n",
      "[[-0.02010742]\n",
      " [ 0.07882023]]\n",
      "[[-0.02011459]\n",
      " [ 0.07879627]]\n",
      "[[-0.02012173]\n",
      " [ 0.07877235]]\n",
      "[[-0.02012883]\n",
      " [ 0.07874846]]\n",
      "[[-0.02013589]\n",
      " [ 0.0787246 ]]\n",
      "[[-0.02014292]\n",
      " [ 0.07870078]]\n",
      "[[-0.02014991]\n",
      " [ 0.078677  ]]\n",
      "[[-0.02015686]\n",
      " [ 0.07865326]]\n",
      "[[-0.02016378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.07862955]]\n",
      "[[-0.02017066]\n",
      " [ 0.07860587]]\n",
      "[[-0.0201775 ]\n",
      " [ 0.07858223]]\n",
      "[[-0.02018431]\n",
      " [ 0.07855863]]\n",
      "[[-0.02019109]\n",
      " [ 0.07853506]]\n",
      "[[-0.02019783]\n",
      " [ 0.07851153]]\n",
      "[[-0.02020453]\n",
      " [ 0.07848803]]\n",
      "[[-0.0202112 ]\n",
      " [ 0.07846457]]\n",
      "[[-0.02021783]\n",
      " [ 0.07844115]]\n",
      "[[-0.02022443]\n",
      " [ 0.07841776]]\n",
      "[[-0.02023099]\n",
      " [ 0.0783944 ]]\n",
      "[[-0.02023752]\n",
      " [ 0.07837108]]\n",
      "[[-0.02024402]\n",
      " [ 0.07834779]]\n",
      "[[-0.02025048]\n",
      " [ 0.07832454]]\n",
      "[[-0.0202569 ]\n",
      " [ 0.07830133]]\n",
      "[[-0.0202633 ]\n",
      " [ 0.07827814]]\n",
      "[[-0.02026965]\n",
      " [ 0.078255  ]]\n",
      "[[-0.02027598]\n",
      " [ 0.07823189]]\n",
      "[[-0.02028227]\n",
      " [ 0.07820881]]\n",
      "[[-0.02028853]\n",
      " [ 0.07818576]]\n",
      "[[-0.02029475]\n",
      " [ 0.07816276]]\n",
      "[[-0.02030094]\n",
      " [ 0.07813978]]\n",
      "[[-0.0203071 ]\n",
      " [ 0.07811684]]\n",
      "[[-0.02031322]\n",
      " [ 0.07809393]]\n",
      "[[-0.02031932]\n",
      " [ 0.07807106]]\n",
      "[[-0.02032537]\n",
      " [ 0.07804822]]\n",
      "[[-0.0203314 ]\n",
      " [ 0.07802542]]\n",
      "[[-0.02033739]\n",
      " [ 0.07800265]]\n",
      "[[-0.02034335]\n",
      " [ 0.07797991]]\n",
      "[[-0.02034928]\n",
      " [ 0.07795721]]\n",
      "[[-0.02035518]\n",
      " [ 0.07793454]]\n",
      "[[-0.02036105]\n",
      " [ 0.0779119 ]]\n",
      "[[-0.02036688]\n",
      " [ 0.0778893 ]]\n",
      "[[-0.02037268]\n",
      " [ 0.07786673]]\n",
      "[[-0.02037845]\n",
      " [ 0.07784419]]\n",
      "[[-0.02038419]\n",
      " [ 0.07782169]]\n",
      "[[-0.02038989]\n",
      " [ 0.07779922]]\n",
      "[[-0.02039557]\n",
      " [ 0.07777678]]\n",
      "[[-0.02040121]\n",
      " [ 0.07775438]]\n",
      "[[-0.02040682]\n",
      " [ 0.07773201]]\n",
      "[[-0.02041241]\n",
      " [ 0.07770967]]\n",
      "[[-0.02041796]\n",
      " [ 0.07768737]]\n",
      "[[-0.02042348]\n",
      " [ 0.0776651 ]]\n",
      "[[-0.02042897]\n",
      " [ 0.07764286]]\n",
      "[[-0.02043443]\n",
      " [ 0.07762065]]\n",
      "[[-0.02043986]\n",
      " [ 0.07759848]]\n",
      "[[-0.02044525]\n",
      " [ 0.07757633]]\n",
      "[[-0.02045062]\n",
      " [ 0.07755422]]\n",
      "[[-0.02045596]\n",
      " [ 0.07753215]]\n",
      "[[-0.02046127]\n",
      " [ 0.0775101 ]]\n",
      "[[-0.02046655]\n",
      " [ 0.07748809]]\n",
      "[[-0.0204718 ]\n",
      " [ 0.07746611]]\n",
      "[[-0.02047702]\n",
      " [ 0.07744416]]\n",
      "[[-0.02048221]\n",
      " [ 0.07742224]]\n",
      "[[-0.02048737]\n",
      " [ 0.07740036]]\n",
      "[[-0.0204925 ]\n",
      " [ 0.07737851]]\n",
      "[[-0.0204976 ]\n",
      " [ 0.07735669]]\n",
      "[[-0.02050268]\n",
      " [ 0.0773349 ]]\n",
      "[[-0.02050772]\n",
      " [ 0.07731314]]\n",
      "[[-0.02051274]\n",
      " [ 0.07729141]]\n",
      "[[-0.02051772]\n",
      " [ 0.07726972]]\n",
      "[[-0.02052268]\n",
      " [ 0.07724805]]\n",
      "[[-0.02052761]\n",
      " [ 0.07722642]]\n",
      "[[-0.02053251]\n",
      " [ 0.07720482]]\n",
      "[[-0.02053739]\n",
      " [ 0.07718325]]\n",
      "[[-0.02054223]\n",
      " [ 0.07716171]]\n",
      "[[-0.02054705]\n",
      " [ 0.07714021]]\n",
      "[[-0.02055184]\n",
      " [ 0.07711873]]\n",
      "[[-0.0205566 ]\n",
      " [ 0.07709728]]\n",
      "[[-0.02056134]\n",
      " [ 0.07707587]]\n",
      "[[-0.02056604]\n",
      " [ 0.07705449]]\n",
      "[[-0.02057072]\n",
      " [ 0.07703313]]\n",
      "[[-0.02057537]\n",
      " [ 0.07701181]]\n",
      "[[-0.02057999]\n",
      " [ 0.07699052]]\n",
      "[[-0.02058459]\n",
      " [ 0.07696926]]\n",
      "[[-0.02058916]\n",
      " [ 0.07694803]]\n",
      "[[-0.0205937 ]\n",
      " [ 0.07692683]]\n",
      "[[-0.02059822]\n",
      " [ 0.07690566]]\n",
      "Cost after iteration 1100: 0.534732\n",
      "[[-0.02060271]\n",
      " [ 0.07688452]]\n",
      "[[-0.02060717]\n",
      " [ 0.07686341]]\n",
      "[[-0.02061161]\n",
      " [ 0.07684233]]\n",
      "[[-0.02061601]\n",
      " [ 0.07682128]]\n",
      "[[-0.0206204 ]\n",
      " [ 0.07680026]]\n",
      "[[-0.02062475]\n",
      " [ 0.07677927]]\n",
      "[[-0.02062908]\n",
      " [ 0.07675831]]\n",
      "[[-0.02063339]\n",
      " [ 0.07673738]]\n",
      "[[-0.02063767]\n",
      " [ 0.07671648]]\n",
      "[[-0.02064192]\n",
      " [ 0.07669561]]\n",
      "[[-0.02064614]\n",
      " [ 0.07667477]]\n",
      "[[-0.02065034]\n",
      " [ 0.07665396]]\n",
      "[[-0.02065452]\n",
      " [ 0.07663317]]\n",
      "[[-0.02065867]\n",
      " [ 0.07661242]]\n",
      "[[-0.02066279]\n",
      " [ 0.0765917 ]]\n",
      "[[-0.02066689]\n",
      " [ 0.076571  ]]\n",
      "[[-0.02067096]\n",
      " [ 0.07655034]]\n",
      "[[-0.02067501]\n",
      " [ 0.0765297 ]]\n",
      "[[-0.02067904]\n",
      " [ 0.0765091 ]]\n",
      "[[-0.02068303]\n",
      " [ 0.07648852]]\n",
      "[[-0.02068701]\n",
      " [ 0.07646797]]\n",
      "[[-0.02069096]\n",
      " [ 0.07644745]]\n",
      "[[-0.02069488]\n",
      " [ 0.07642696]]\n",
      "[[-0.02069878]\n",
      " [ 0.0764065 ]]\n",
      "[[-0.02070265]\n",
      " [ 0.07638606]]\n",
      "[[-0.0207065 ]\n",
      " [ 0.07636566]]\n",
      "[[-0.02071033]\n",
      " [ 0.07634528]]\n",
      "[[-0.02071413]\n",
      " [ 0.07632493]]\n",
      "[[-0.02071791]\n",
      " [ 0.07630461]]\n",
      "[[-0.02072166]\n",
      " [ 0.07628432]]\n",
      "[[-0.02072539]\n",
      " [ 0.07626406]]\n",
      "[[-0.0207291 ]\n",
      " [ 0.07624383]]\n",
      "[[-0.02073278]\n",
      " [ 0.07622362]]\n",
      "[[-0.02073644]\n",
      " [ 0.07620344]]\n",
      "[[-0.02074007]\n",
      " [ 0.07618329]]\n",
      "[[-0.02074368]\n",
      " [ 0.07616317]]\n",
      "[[-0.02074727]\n",
      " [ 0.07614307]]\n",
      "[[-0.02075084]\n",
      " [ 0.07612301]]\n",
      "[[-0.02075438]\n",
      " [ 0.07610297]]\n",
      "[[-0.02075789]\n",
      " [ 0.07608296]]\n",
      "[[-0.02076139]\n",
      " [ 0.07606297]]\n",
      "[[-0.02076486]\n",
      " [ 0.07604302]]\n",
      "[[-0.02076831]\n",
      " [ 0.07602309]]\n",
      "[[-0.02077173]\n",
      " [ 0.07600319]]\n",
      "[[-0.02077514]\n",
      " [ 0.07598332]]\n",
      "[[-0.02077852]\n",
      " [ 0.07596347]]\n",
      "[[-0.02078188]\n",
      " [ 0.07594365]]\n",
      "[[-0.02078521]\n",
      " [ 0.07592386]]\n",
      "[[-0.02078852]\n",
      " [ 0.0759041 ]]\n",
      "[[-0.02079181]\n",
      " [ 0.07588436]]\n",
      "[[-0.02079508]\n",
      " [ 0.07586465]]\n",
      "[[-0.02079833]\n",
      " [ 0.07584497]]\n",
      "[[-0.02080155]\n",
      " [ 0.07582531]]\n",
      "[[-0.02080475]\n",
      " [ 0.07580568]]\n",
      "[[-0.02080793]\n",
      " [ 0.07578608]]\n",
      "[[-0.02081109]\n",
      " [ 0.0757665 ]]\n",
      "[[-0.02081423]\n",
      " [ 0.07574696]]\n",
      "[[-0.02081734]\n",
      " [ 0.07572743]]\n",
      "[[-0.02082044]\n",
      " [ 0.07570794]]\n",
      "[[-0.02082351]\n",
      " [ 0.07568847]]\n",
      "[[-0.02082656]\n",
      " [ 0.07566903]]\n",
      "[[-0.02082959]\n",
      " [ 0.07564961]]\n",
      "[[-0.02083259]\n",
      " [ 0.07563022]]\n",
      "[[-0.02083558]\n",
      " [ 0.07561086]]\n",
      "[[-0.02083855]\n",
      " [ 0.07559152]]\n",
      "[[-0.02084149]\n",
      " [ 0.07557221]]\n",
      "[[-0.02084441]\n",
      " [ 0.07555292]]\n",
      "[[-0.02084731]\n",
      " [ 0.07553366]]\n",
      "[[-0.0208502 ]\n",
      " [ 0.07551443]]\n",
      "[[-0.02085306]\n",
      " [ 0.07549522]]\n",
      "[[-0.0208559 ]\n",
      " [ 0.07547604]]\n",
      "[[-0.02085871]\n",
      " [ 0.07545689]]\n",
      "[[-0.02086151]\n",
      " [ 0.07543776]]\n",
      "[[-0.02086429]\n",
      " [ 0.07541865]]\n",
      "[[-0.02086705]\n",
      " [ 0.07539957]]\n",
      "[[-0.02086979]\n",
      " [ 0.07538052]]\n",
      "[[-0.0208725 ]\n",
      " [ 0.07536149]]\n",
      "[[-0.0208752 ]\n",
      " [ 0.07534249]]\n",
      "[[-0.02087788]\n",
      " [ 0.07532352]]\n",
      "[[-0.02088053]\n",
      " [ 0.07530456]]\n",
      "[[-0.02088317]\n",
      " [ 0.07528564]]\n",
      "[[-0.02088579]\n",
      " [ 0.07526674]]\n",
      "[[-0.02088839]\n",
      " [ 0.07524786]]\n",
      "[[-0.02089096]\n",
      " [ 0.07522901]]\n",
      "[[-0.02089352]\n",
      " [ 0.07521019]]\n",
      "[[-0.02089606]\n",
      " [ 0.07519139]]\n",
      "[[-0.02089858]\n",
      " [ 0.07517261]]\n",
      "[[-0.02090108]\n",
      " [ 0.07515386]]\n",
      "[[-0.02090356]\n",
      " [ 0.07513514]]\n",
      "[[-0.02090602]\n",
      " [ 0.07511644]]\n",
      "[[-0.02090846]\n",
      " [ 0.07509776]]\n",
      "[[-0.02091088]\n",
      " [ 0.07507911]]\n",
      "[[-0.02091328]\n",
      " [ 0.07506048]]\n",
      "[[-0.02091567]\n",
      " [ 0.07504188]]\n",
      "[[-0.02091803]\n",
      " [ 0.0750233 ]]\n",
      "[[-0.02092038]\n",
      " [ 0.07500475]]\n",
      "[[-0.02092271]\n",
      " [ 0.07498622]]\n",
      "[[-0.02092501]\n",
      " [ 0.07496771]]\n",
      "[[-0.0209273 ]\n",
      " [ 0.07494923]]\n",
      "[[-0.02092957]\n",
      " [ 0.07493078]]\n",
      "Cost after iteration 1200: 0.524570\n",
      "[[-0.02093183]\n",
      " [ 0.07491235]]\n",
      "[[-0.02093406]\n",
      " [ 0.07489394]]\n",
      "[[-0.02093628]\n",
      " [ 0.07487555]]\n",
      "[[-0.02093847]\n",
      " [ 0.0748572 ]]\n",
      "[[-0.02094065]\n",
      " [ 0.07483886]]\n",
      "[[-0.02094281]\n",
      " [ 0.07482055]]\n",
      "[[-0.02094496]\n",
      " [ 0.07480226]]\n",
      "[[-0.02094708]\n",
      " [ 0.074784  ]]\n",
      "[[-0.02094919]\n",
      " [ 0.07476575]]\n",
      "[[-0.02095128]\n",
      " [ 0.07474754]]\n",
      "[[-0.02095335]\n",
      " [ 0.07472935]]\n",
      "[[-0.0209554 ]\n",
      " [ 0.07471118]]\n",
      "[[-0.02095744]\n",
      " [ 0.07469303]]\n",
      "[[-0.02095945]\n",
      " [ 0.07467491]]\n",
      "[[-0.02096145]\n",
      " [ 0.07465681]]\n",
      "[[-0.02096343]\n",
      " [ 0.07463873]]\n",
      "[[-0.0209654 ]\n",
      " [ 0.07462068]]\n",
      "[[-0.02096735]\n",
      " [ 0.07460265]]\n",
      "[[-0.02096928]\n",
      " [ 0.07458464]]\n",
      "[[-0.02097119]\n",
      " [ 0.07456666]]\n",
      "[[-0.02097308]\n",
      " [ 0.0745487 ]]\n",
      "[[-0.02097496]\n",
      " [ 0.07453076]]\n",
      "[[-0.02097682]\n",
      " [ 0.07451285]]\n",
      "[[-0.02097867]\n",
      " [ 0.07449496]]\n",
      "[[-0.02098049]\n",
      " [ 0.07447709]]\n",
      "[[-0.0209823 ]\n",
      " [ 0.07445925]]\n",
      "[[-0.0209841 ]\n",
      " [ 0.07444142]]\n",
      "[[-0.02098587]\n",
      " [ 0.07442362]]\n",
      "[[-0.02098763]\n",
      " [ 0.07440585]]\n",
      "[[-0.02098937]\n",
      " [ 0.07438809]]\n",
      "[[-0.0209911 ]\n",
      " [ 0.07437036]]\n",
      "[[-0.02099281]\n",
      " [ 0.07435265]]\n",
      "[[-0.0209945 ]\n",
      " [ 0.07433496]]\n",
      "[[-0.02099618]\n",
      " [ 0.0743173 ]]\n",
      "[[-0.02099784]\n",
      " [ 0.07429966]]\n",
      "[[-0.02099948]\n",
      " [ 0.07428204]]\n",
      "[[-0.02100111]\n",
      " [ 0.07426444]]\n",
      "[[-0.02100272]\n",
      " [ 0.07424687]]\n",
      "[[-0.02100431]\n",
      " [ 0.07422931]]\n",
      "[[-0.02100589]\n",
      " [ 0.07421178]]\n",
      "[[-0.02100745]\n",
      " [ 0.07419427]]\n",
      "[[-0.021009  ]\n",
      " [ 0.07417679]]\n",
      "[[-0.02101053]\n",
      " [ 0.07415932]]\n",
      "[[-0.02101205]\n",
      " [ 0.07414188]]\n",
      "[[-0.02101354]\n",
      " [ 0.07412446]]\n",
      "[[-0.02101503]\n",
      " [ 0.07410706]]\n",
      "[[-0.02101649]\n",
      " [ 0.07408968]]\n",
      "[[-0.02101795]\n",
      " [ 0.07407232]]\n",
      "[[-0.02101938]\n",
      " [ 0.07405499]]\n",
      "[[-0.0210208 ]\n",
      " [ 0.07403768]]\n",
      "[[-0.02102221]\n",
      " [ 0.07402039]]\n",
      "[[-0.0210236 ]\n",
      " [ 0.07400312]]\n",
      "[[-0.02102497]\n",
      " [ 0.07398587]]\n",
      "[[-0.02102633]\n",
      " [ 0.07396864]]\n",
      "[[-0.02102767]\n",
      " [ 0.07395144]]\n",
      "[[-0.021029  ]\n",
      " [ 0.07393425]]\n",
      "[[-0.02103031]\n",
      " [ 0.07391709]]\n",
      "[[-0.02103161]\n",
      " [ 0.07389995]]\n",
      "[[-0.02103289]\n",
      " [ 0.07388283]]\n",
      "[[-0.02103416]\n",
      " [ 0.07386573]]\n",
      "[[-0.02103541]\n",
      " [ 0.07384865]]\n",
      "[[-0.02103665]\n",
      " [ 0.07383159]]\n",
      "[[-0.02103787]\n",
      " [ 0.07381456]]\n",
      "[[-0.02103908]\n",
      " [ 0.07379754]]\n",
      "[[-0.02104028]\n",
      " [ 0.07378055]]\n",
      "[[-0.02104145]\n",
      " [ 0.07376358]]\n",
      "[[-0.02104262]\n",
      " [ 0.07374662]]\n",
      "[[-0.02104377]\n",
      " [ 0.07372969]]\n",
      "[[-0.0210449 ]\n",
      " [ 0.07371278]]\n",
      "[[-0.02104602]\n",
      " [ 0.07369589]]\n",
      "[[-0.02104713]\n",
      " [ 0.07367902]]\n",
      "[[-0.02104822]\n",
      " [ 0.07366217]]\n",
      "[[-0.0210493 ]\n",
      " [ 0.07364534]]\n",
      "[[-0.02105036]\n",
      " [ 0.07362853]]\n",
      "[[-0.02105141]\n",
      " [ 0.07361174]]\n",
      "[[-0.02105244]\n",
      " [ 0.07359498]]\n",
      "[[-0.02105346]\n",
      " [ 0.07357823]]\n",
      "[[-0.02105447]\n",
      " [ 0.0735615 ]]\n",
      "[[-0.02105546]\n",
      " [ 0.07354479]]\n",
      "[[-0.02105644]\n",
      " [ 0.07352811]]\n",
      "[[-0.0210574 ]\n",
      " [ 0.07351144]]\n",
      "[[-0.02105835]\n",
      " [ 0.07349479]]\n",
      "[[-0.02105929]\n",
      " [ 0.07347817]]\n",
      "[[-0.02106021]\n",
      " [ 0.07346156]]\n",
      "[[-0.02106112]\n",
      " [ 0.07344498]]\n",
      "[[-0.02106201]\n",
      " [ 0.07342841]]\n",
      "[[-0.0210629 ]\n",
      " [ 0.07341186]]\n",
      "[[-0.02106376]\n",
      " [ 0.07339534]]\n",
      "[[-0.02106462]\n",
      " [ 0.07337883]]\n",
      "[[-0.02106546]\n",
      " [ 0.07336234]]\n",
      "[[-0.02106629]\n",
      " [ 0.07334587]]\n",
      "[[-0.0210671 ]\n",
      " [ 0.07332942]]\n",
      "[[-0.0210679]\n",
      " [ 0.073313 ]]\n",
      "[[-0.02106869]\n",
      " [ 0.07329659]]\n",
      "[[-0.02106946]\n",
      " [ 0.0732802 ]]\n",
      "[[-0.02107022]\n",
      " [ 0.07326383]]\n",
      "[[-0.02107097]\n",
      " [ 0.07324748]]\n",
      "[[-0.0210717 ]\n",
      " [ 0.07323114]]\n",
      "[[-0.02107242]\n",
      " [ 0.07321483]]\n",
      "[[-0.02107313]\n",
      " [ 0.07319854]]\n",
      "Cost after iteration 1300: 0.514696\n",
      "[[-0.02107383]\n",
      " [ 0.07318227]]\n",
      "[[-0.02107451]\n",
      " [ 0.07316601]]\n",
      "[[-0.02107518]\n",
      " [ 0.07314978]]\n",
      "[[-0.02107584]\n",
      " [ 0.07313356]]\n",
      "[[-0.02107648]\n",
      " [ 0.07311736]]\n",
      "[[-0.02107711]\n",
      " [ 0.07310119]]\n",
      "[[-0.02107773]\n",
      " [ 0.07308503]]\n",
      "[[-0.02107833]\n",
      " [ 0.07306889]]\n",
      "[[-0.02107893]\n",
      " [ 0.07305276]]\n",
      "[[-0.02107951]\n",
      " [ 0.07303666]]\n",
      "[[-0.02108007]\n",
      " [ 0.07302058]]\n",
      "[[-0.02108063]\n",
      " [ 0.07300451]]\n",
      "[[-0.02108117]\n",
      " [ 0.07298847]]\n",
      "[[-0.0210817 ]\n",
      " [ 0.07297244]]\n",
      "[[-0.02108222]\n",
      " [ 0.07295643]]\n",
      "[[-0.02108272]\n",
      " [ 0.07294044]]\n",
      "[[-0.02108322]\n",
      " [ 0.07292447]]\n",
      "[[-0.0210837 ]\n",
      " [ 0.07290851]]\n",
      "[[-0.02108417]\n",
      " [ 0.07289258]]\n",
      "[[-0.02108462]\n",
      " [ 0.07287666]]\n",
      "[[-0.02108507]\n",
      " [ 0.07286077]]\n",
      "[[-0.0210855 ]\n",
      " [ 0.07284489]]\n",
      "[[-0.02108592]\n",
      " [ 0.07282902]]\n",
      "[[-0.02108633]\n",
      " [ 0.07281318]]\n",
      "[[-0.02108672]\n",
      " [ 0.07279736]]\n",
      "[[-0.02108711]\n",
      " [ 0.07278155]]\n",
      "[[-0.02108748]\n",
      " [ 0.07276576]]\n",
      "[[-0.02108784]\n",
      " [ 0.07274999]]\n",
      "[[-0.02108819]\n",
      " [ 0.07273424]]\n",
      "[[-0.02108853]\n",
      " [ 0.0727185 ]]\n",
      "[[-0.02108885]\n",
      " [ 0.07270278]]\n",
      "[[-0.02108917]\n",
      " [ 0.07268708]]\n",
      "[[-0.02108947]\n",
      " [ 0.0726714 ]]\n",
      "[[-0.02108976]\n",
      " [ 0.07265574]]\n",
      "[[-0.02109004]\n",
      " [ 0.07264009]]\n",
      "[[-0.02109031]\n",
      " [ 0.07262447]]\n",
      "[[-0.02109056]\n",
      " [ 0.07260886]]\n",
      "[[-0.02109081]\n",
      " [ 0.07259326]]\n",
      "[[-0.02109104]\n",
      " [ 0.07257769]]\n",
      "[[-0.02109126]\n",
      " [ 0.07256213]]\n",
      "[[-0.02109147]\n",
      " [ 0.07254659]]\n",
      "[[-0.02109167]\n",
      " [ 0.07253107]]\n",
      "[[-0.02109186]\n",
      " [ 0.07251557]]\n",
      "[[-0.02109204]\n",
      " [ 0.07250008]]\n",
      "[[-0.0210922 ]\n",
      " [ 0.07248461]]\n",
      "[[-0.02109236]\n",
      " [ 0.07246916]]\n",
      "[[-0.0210925 ]\n",
      " [ 0.07245372]]\n",
      "[[-0.02109263]\n",
      " [ 0.0724383 ]]\n",
      "[[-0.02109276]\n",
      " [ 0.0724229 ]]\n",
      "[[-0.02109287]\n",
      " [ 0.07240752]]\n",
      "[[-0.02109297]\n",
      " [ 0.07239215]]\n",
      "[[-0.02109305]\n",
      " [ 0.0723768 ]]\n",
      "[[-0.02109313]\n",
      " [ 0.07236147]]\n",
      "[[-0.0210932 ]\n",
      " [ 0.07234615]]\n",
      "[[-0.02109326]\n",
      " [ 0.07233086]]\n",
      "[[-0.0210933 ]\n",
      " [ 0.07231557]]\n",
      "[[-0.02109334]\n",
      " [ 0.07230031]]\n",
      "[[-0.02109336]\n",
      " [ 0.07228506]]\n",
      "[[-0.02109337]\n",
      " [ 0.07226983]]\n",
      "[[-0.02109338]\n",
      " [ 0.07225462]]\n",
      "[[-0.02109337]\n",
      " [ 0.07223942]]\n",
      "[[-0.02109335]\n",
      " [ 0.07222424]]\n",
      "[[-0.02109332]\n",
      " [ 0.07220908]]\n",
      "[[-0.02109329]\n",
      " [ 0.07219393]]\n",
      "[[-0.02109324]\n",
      " [ 0.0721788 ]]\n",
      "[[-0.02109318]\n",
      " [ 0.07216368]]\n",
      "[[-0.02109311]\n",
      " [ 0.07214859]]\n",
      "[[-0.02109303]\n",
      " [ 0.0721335 ]]\n",
      "[[-0.02109294]\n",
      " [ 0.07211844]]\n",
      "[[-0.02109283]\n",
      " [ 0.07210339]]\n",
      "[[-0.02109272]\n",
      " [ 0.07208836]]\n",
      "[[-0.0210926 ]\n",
      " [ 0.07207334]]\n",
      "[[-0.02109247]\n",
      " [ 0.07205834]]\n",
      "[[-0.02109233]\n",
      " [ 0.07204336]]\n",
      "[[-0.02109218]\n",
      " [ 0.07202839]]\n",
      "[[-0.02109202]\n",
      " [ 0.07201344]]\n",
      "[[-0.02109185]\n",
      " [ 0.07199851]]\n",
      "[[-0.02109167]\n",
      " [ 0.07198359]]\n",
      "[[-0.02109147]\n",
      " [ 0.07196869]]\n",
      "[[-0.02109127]\n",
      " [ 0.0719538 ]]\n",
      "[[-0.02109106]\n",
      " [ 0.07193893]]\n",
      "[[-0.02109084]\n",
      " [ 0.07192408]]\n",
      "[[-0.02109061]\n",
      " [ 0.07190924]]\n",
      "[[-0.02109037]\n",
      " [ 0.07189442]]\n",
      "[[-0.02109012]\n",
      " [ 0.07187961]]\n",
      "[[-0.02108986]\n",
      " [ 0.07186482]]\n",
      "[[-0.02108959]\n",
      " [ 0.07185004]]\n",
      "[[-0.02108931]\n",
      " [ 0.07183528]]\n",
      "[[-0.02108903]\n",
      " [ 0.07182054]]\n",
      "[[-0.02108873]\n",
      " [ 0.07180581]]\n",
      "[[-0.02108842]\n",
      " [ 0.0717911 ]]\n",
      "[[-0.0210881]\n",
      " [ 0.0717764]]\n",
      "[[-0.02108778]\n",
      " [ 0.07176172]]\n",
      "[[-0.02108744]\n",
      " [ 0.07174706]]\n",
      "[[-0.0210871 ]\n",
      " [ 0.07173241]]\n",
      "[[-0.02108674]\n",
      " [ 0.07171777]]\n",
      "[[-0.02108638]\n",
      " [ 0.07170315]]\n",
      "[[-0.021086  ]\n",
      " [ 0.07168855]]\n",
      "[[-0.02108562]\n",
      " [ 0.07167396]]\n",
      "[[-0.02108523]\n",
      " [ 0.07165939]]\n",
      "Cost after iteration 1400: 0.505099\n",
      "[[-0.02108483]\n",
      " [ 0.07164483]]\n",
      "[[-0.02108442]\n",
      " [ 0.07163028]]\n",
      "[[-0.021084  ]\n",
      " [ 0.07161576]]\n",
      "[[-0.02108357]\n",
      " [ 0.07160124]]\n",
      "[[-0.02108314]\n",
      " [ 0.07158675]]\n",
      "[[-0.02108269]\n",
      " [ 0.07157226]]\n",
      "[[-0.02108224]\n",
      " [ 0.0715578 ]]\n",
      "[[-0.02108177]\n",
      " [ 0.07154335]]\n",
      "[[-0.0210813 ]\n",
      " [ 0.07152891]]\n",
      "[[-0.02108082]\n",
      " [ 0.07151449]]\n",
      "[[-0.02108033]\n",
      " [ 0.07150008]]\n",
      "[[-0.02107983]\n",
      " [ 0.07148569]]\n",
      "[[-0.02107932]\n",
      " [ 0.07147131]]\n",
      "[[-0.0210788 ]\n",
      " [ 0.07145695]]\n",
      "[[-0.02107827]\n",
      " [ 0.0714426 ]]\n",
      "[[-0.02107774]\n",
      " [ 0.07142827]]\n",
      "[[-0.0210772 ]\n",
      " [ 0.07141395]]\n",
      "[[-0.02107664]\n",
      " [ 0.07139964]]\n",
      "[[-0.02107608]\n",
      " [ 0.07138535]]\n",
      "[[-0.02107551]\n",
      " [ 0.07137108]]\n",
      "[[-0.02107494]\n",
      " [ 0.07135682]]\n",
      "[[-0.02107435]\n",
      " [ 0.07134257]]\n",
      "[[-0.02107376]\n",
      " [ 0.07132834]]\n",
      "[[-0.02107315]\n",
      " [ 0.07131413]]\n",
      "[[-0.02107254]\n",
      " [ 0.07129993]]\n",
      "[[-0.02107192]\n",
      " [ 0.07128574]]\n",
      "[[-0.02107129]\n",
      " [ 0.07127157]]\n",
      "[[-0.02107065]\n",
      " [ 0.07125741]]\n",
      "[[-0.02107001]\n",
      " [ 0.07124326]]\n",
      "[[-0.02106936]\n",
      " [ 0.07122913]]\n",
      "[[-0.02106869]\n",
      " [ 0.07121502]]\n",
      "[[-0.02106802]\n",
      " [ 0.07120092]]\n",
      "[[-0.02106735]\n",
      " [ 0.07118683]]\n",
      "[[-0.02106666]\n",
      " [ 0.07117276]]\n",
      "[[-0.02106596]\n",
      " [ 0.0711587 ]]\n",
      "[[-0.02106526]\n",
      " [ 0.07114465]]\n",
      "[[-0.02106455]\n",
      " [ 0.07113062]]\n",
      "[[-0.02106383]\n",
      " [ 0.0711166 ]]\n",
      "[[-0.0210631]\n",
      " [ 0.0711026]]\n",
      "[[-0.02106237]\n",
      " [ 0.07108861]]\n",
      "[[-0.02106163]\n",
      " [ 0.07107464]]\n",
      "[[-0.02106087]\n",
      " [ 0.07106068]]\n",
      "[[-0.02106012]\n",
      " [ 0.07104673]]\n",
      "[[-0.02105935]\n",
      " [ 0.0710328 ]]\n",
      "[[-0.02105857]\n",
      " [ 0.07101888]]\n",
      "[[-0.02105779]\n",
      " [ 0.07100497]]\n",
      "[[-0.021057  ]\n",
      " [ 0.07099108]]\n",
      "[[-0.0210562]\n",
      " [ 0.0709772]]\n",
      "[[-0.0210554 ]\n",
      " [ 0.07096334]]\n",
      "[[-0.02105458]\n",
      " [ 0.07094948]]\n",
      "[[-0.02105376]\n",
      " [ 0.07093565]]\n",
      "[[-0.02105293]\n",
      " [ 0.07092182]]\n",
      "[[-0.02105209]\n",
      " [ 0.07090801]]\n",
      "[[-0.02105125]\n",
      " [ 0.07089422]]\n",
      "[[-0.0210504 ]\n",
      " [ 0.07088043]]\n",
      "[[-0.02104954]\n",
      " [ 0.07086666]]\n",
      "[[-0.02104867]\n",
      " [ 0.07085291]]\n",
      "[[-0.02104779]\n",
      " [ 0.07083916]]\n",
      "[[-0.02104691]\n",
      " [ 0.07082544]]\n",
      "[[-0.02104602]\n",
      " [ 0.07081172]]\n",
      "[[-0.02104513]\n",
      " [ 0.07079802]]\n",
      "[[-0.02104422]\n",
      " [ 0.07078433]]\n",
      "[[-0.02104331]\n",
      " [ 0.07077065]]\n",
      "[[-0.02104239]\n",
      " [ 0.07075699]]\n",
      "[[-0.02104146]\n",
      " [ 0.07074334]]\n",
      "[[-0.02104053]\n",
      " [ 0.0707297 ]]\n",
      "[[-0.02103959]\n",
      " [ 0.07071608]]\n",
      "[[-0.02103864]\n",
      " [ 0.07070247]]\n",
      "[[-0.02103768]\n",
      " [ 0.07068887]]\n",
      "[[-0.02103672]\n",
      " [ 0.07067529]]\n",
      "[[-0.02103575]\n",
      " [ 0.07066171]]\n",
      "[[-0.02103477]\n",
      " [ 0.07064816]]\n",
      "[[-0.02103378]\n",
      " [ 0.07063461]]\n",
      "[[-0.02103279]\n",
      " [ 0.07062108]]\n",
      "[[-0.02103179]\n",
      " [ 0.07060756]]\n",
      "[[-0.02103078]\n",
      " [ 0.07059405]]\n",
      "[[-0.02102977]\n",
      " [ 0.07058056]]\n",
      "[[-0.02102875]\n",
      " [ 0.07056708]]\n",
      "[[-0.02102772]\n",
      " [ 0.07055361]]\n",
      "[[-0.02102669]\n",
      " [ 0.07054015]]\n",
      "[[-0.02102565]\n",
      " [ 0.07052671]]\n",
      "[[-0.0210246 ]\n",
      " [ 0.07051328]]\n",
      "[[-0.02102354]\n",
      " [ 0.07049986]]\n",
      "[[-0.02102248]\n",
      " [ 0.07048646]]\n",
      "[[-0.02102141]\n",
      " [ 0.07047307]]\n",
      "[[-0.02102034]\n",
      " [ 0.07045969]]\n",
      "[[-0.02101925]\n",
      " [ 0.07044632]]\n",
      "[[-0.02101816]\n",
      " [ 0.07043296]]\n",
      "[[-0.02101707]\n",
      " [ 0.07041962]]\n",
      "[[-0.02101596]\n",
      " [ 0.07040629]]\n",
      "[[-0.02101485]\n",
      " [ 0.07039298]]\n",
      "[[-0.02101374]\n",
      " [ 0.07037967]]\n",
      "[[-0.02101261]\n",
      " [ 0.07036638]]\n",
      "[[-0.02101148]\n",
      " [ 0.0703531 ]]\n",
      "[[-0.02101035]\n",
      " [ 0.07033983]]\n",
      "[[-0.0210092 ]\n",
      " [ 0.07032658]]\n",
      "[[-0.02100806]\n",
      " [ 0.07031333]]\n",
      "[[-0.0210069]\n",
      " [ 0.0703001]]\n",
      "[[-0.02100574]\n",
      " [ 0.07028688]]\n",
      "[[-0.02100457]\n",
      " [ 0.07027368]]\n",
      "Cost after iteration 1500: 0.495769\n",
      "[[-0.02100339]\n",
      " [ 0.07026048]]\n",
      "[[-0.02100221]\n",
      " [ 0.0702473 ]]\n",
      "[[-0.02100102]\n",
      " [ 0.07023413]]\n",
      "[[-0.02099982]\n",
      " [ 0.07022097]]\n",
      "[[-0.02099862]\n",
      " [ 0.07020783]]\n",
      "[[-0.02099741]\n",
      " [ 0.07019469]]\n",
      "[[-0.0209962 ]\n",
      " [ 0.07018157]]\n",
      "[[-0.02099498]\n",
      " [ 0.07016846]]\n",
      "[[-0.02099375]\n",
      " [ 0.07015536]]\n",
      "[[-0.02099252]\n",
      " [ 0.07014228]]\n",
      "[[-0.02099128]\n",
      " [ 0.0701292 ]]\n",
      "[[-0.02099003]\n",
      " [ 0.07011614]]\n",
      "[[-0.02098878]\n",
      " [ 0.07010309]]\n",
      "[[-0.02098752]\n",
      " [ 0.07009005]]\n",
      "[[-0.02098626]\n",
      " [ 0.07007702]]\n",
      "[[-0.02098499]\n",
      " [ 0.07006401]]\n",
      "[[-0.02098371]\n",
      " [ 0.070051  ]]\n",
      "[[-0.02098243]\n",
      " [ 0.07003801]]\n",
      "[[-0.02098114]\n",
      " [ 0.07002503]]\n",
      "[[-0.02097984]\n",
      " [ 0.07001206]]\n",
      "[[-0.02097854]\n",
      " [ 0.06999911]]\n",
      "[[-0.02097723]\n",
      " [ 0.06998616]]\n",
      "[[-0.02097592]\n",
      " [ 0.06997323]]\n",
      "[[-0.0209746]\n",
      " [ 0.0699603]]\n",
      "[[-0.02097327]\n",
      " [ 0.06994739]]\n",
      "[[-0.02097194]\n",
      " [ 0.06993449]]\n",
      "[[-0.0209706 ]\n",
      " [ 0.06992161]]\n",
      "[[-0.02096926]\n",
      " [ 0.06990873]]\n",
      "[[-0.02096791]\n",
      " [ 0.06989586]]\n",
      "[[-0.02096656]\n",
      " [ 0.06988301]]\n",
      "[[-0.0209652 ]\n",
      " [ 0.06987017]]\n",
      "[[-0.02096383]\n",
      " [ 0.06985734]]\n",
      "[[-0.02096246]\n",
      " [ 0.06984452]]\n",
      "[[-0.02096108]\n",
      " [ 0.06983171]]\n",
      "[[-0.02095969]\n",
      " [ 0.06981891]]\n",
      "[[-0.0209583 ]\n",
      " [ 0.06980613]]\n",
      "[[-0.02095691]\n",
      " [ 0.06979335]]\n",
      "[[-0.02095551]\n",
      " [ 0.06978059]]\n",
      "[[-0.0209541 ]\n",
      " [ 0.06976784]]\n",
      "[[-0.02095269]\n",
      " [ 0.0697551 ]]\n",
      "[[-0.02095127]\n",
      " [ 0.06974237]]\n",
      "[[-0.02094984]\n",
      " [ 0.06972965]]\n",
      "[[-0.02094841]\n",
      " [ 0.06971694]]\n",
      "[[-0.02094698]\n",
      " [ 0.06970424]]\n",
      "[[-0.02094554]\n",
      " [ 0.06969156]]\n",
      "[[-0.02094409]\n",
      " [ 0.06967888]]\n",
      "[[-0.02094264]\n",
      " [ 0.06966622]]\n",
      "[[-0.02094118]\n",
      " [ 0.06965356]]\n",
      "[[-0.02093972]\n",
      " [ 0.06964092]]\n",
      "[[-0.02093825]\n",
      " [ 0.06962829]]\n",
      "[[-0.02093678]\n",
      " [ 0.06961567]]\n",
      "[[-0.0209353 ]\n",
      " [ 0.06960306]]\n",
      "[[-0.02093381]\n",
      " [ 0.06959046]]\n",
      "[[-0.02093232]\n",
      " [ 0.06957787]]\n",
      "[[-0.02093082]\n",
      " [ 0.0695653 ]]\n",
      "[[-0.02092932]\n",
      " [ 0.06955273]]\n",
      "[[-0.02092782]\n",
      " [ 0.06954017]]\n",
      "[[-0.0209263 ]\n",
      " [ 0.06952763]]\n",
      "[[-0.02092479]\n",
      " [ 0.06951509]]\n",
      "[[-0.02092327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.06950257]]\n",
      "[[-0.02092174]\n",
      " [ 0.06949006]]\n",
      "[[-0.0209202 ]\n",
      " [ 0.06947756]]\n",
      "[[-0.02091867]\n",
      " [ 0.06946506]]\n",
      "[[-0.02091712]\n",
      " [ 0.06945258]]\n",
      "[[-0.02091557]\n",
      " [ 0.06944011]]\n",
      "[[-0.02091402]\n",
      " [ 0.06942765]]\n",
      "[[-0.02091246]\n",
      " [ 0.0694152 ]]\n",
      "[[-0.0209109 ]\n",
      " [ 0.06940276]]\n",
      "[[-0.02090933]\n",
      " [ 0.06939033]]\n",
      "[[-0.02090775]\n",
      " [ 0.06937791]]\n",
      "[[-0.02090617]\n",
      " [ 0.0693655 ]]\n",
      "[[-0.02090459]\n",
      " [ 0.06935311]]\n",
      "[[-0.020903  ]\n",
      " [ 0.06934072]]\n",
      "[[-0.0209014 ]\n",
      " [ 0.06932834]]\n",
      "[[-0.0208998 ]\n",
      " [ 0.06931598]]\n",
      "[[-0.0208982 ]\n",
      " [ 0.06930362]]\n",
      "[[-0.02089659]\n",
      " [ 0.06929127]]\n",
      "[[-0.02089497]\n",
      " [ 0.06927894]]\n",
      "[[-0.02089335]\n",
      " [ 0.06926661]]\n",
      "[[-0.02089173]\n",
      " [ 0.06925429]]\n",
      "[[-0.0208901 ]\n",
      " [ 0.06924199]]\n",
      "[[-0.02088846]\n",
      " [ 0.06922969]]\n",
      "[[-0.02088682]\n",
      " [ 0.06921741]]\n",
      "[[-0.02088518]\n",
      " [ 0.06920513]]\n",
      "[[-0.02088353]\n",
      " [ 0.06919287]]\n",
      "[[-0.02088188]\n",
      " [ 0.06918061]]\n",
      "[[-0.02088022]\n",
      " [ 0.06916837]]\n",
      "[[-0.02087855]\n",
      " [ 0.06915613]]\n",
      "[[-0.02087688]\n",
      " [ 0.0691439 ]]\n",
      "[[-0.02087521]\n",
      " [ 0.06913169]]\n",
      "[[-0.02087353]\n",
      " [ 0.06911948]]\n",
      "[[-0.02087185]\n",
      " [ 0.06910729]]\n",
      "[[-0.02087016]\n",
      " [ 0.0690951 ]]\n",
      "[[-0.02086847]\n",
      " [ 0.06908293]]\n",
      "[[-0.02086677]\n",
      " [ 0.06907076]]\n",
      "[[-0.02086507]\n",
      " [ 0.0690586 ]]\n",
      "[[-0.02086337]\n",
      " [ 0.06904646]]\n",
      "[[-0.02086165]\n",
      " [ 0.06903432]]\n",
      "[[-0.02085994]\n",
      " [ 0.06902219]]\n",
      "[[-0.02085822]\n",
      " [ 0.06901008]]\n",
      "Cost after iteration 1600: 0.486698\n",
      "[[-0.02085649]\n",
      " [ 0.06899797]]\n",
      "[[-0.02085476]\n",
      " [ 0.06898587]]\n",
      "[[-0.02085303]\n",
      " [ 0.06897378]]\n",
      "[[-0.02085129]\n",
      " [ 0.06896171]]\n",
      "[[-0.02084955]\n",
      " [ 0.06894964]]\n",
      "[[-0.0208478 ]\n",
      " [ 0.06893758]]\n",
      "[[-0.02084605]\n",
      " [ 0.06892553]]\n",
      "[[-0.02084429]\n",
      " [ 0.06891349]]\n",
      "[[-0.02084253]\n",
      " [ 0.06890146]]\n",
      "[[-0.02084077]\n",
      " [ 0.06888944]]\n",
      "[[-0.020839  ]\n",
      " [ 0.06887742]]\n",
      "[[-0.02083722]\n",
      " [ 0.06886542]]\n",
      "[[-0.02083544]\n",
      " [ 0.06885343]]\n",
      "[[-0.02083366]\n",
      " [ 0.06884145]]\n",
      "[[-0.02083187]\n",
      " [ 0.06882947]]\n",
      "[[-0.02083008]\n",
      " [ 0.06881751]]\n",
      "[[-0.02082828]\n",
      " [ 0.06880555]]\n",
      "[[-0.02082648]\n",
      " [ 0.06879361]]\n",
      "[[-0.02082468]\n",
      " [ 0.06878167]]\n",
      "[[-0.02082287]\n",
      " [ 0.06876974]]\n",
      "[[-0.02082105]\n",
      " [ 0.06875783]]\n",
      "[[-0.02081924]\n",
      " [ 0.06874592]]\n",
      "[[-0.02081741]\n",
      " [ 0.06873402]]\n",
      "[[-0.02081559]\n",
      " [ 0.06872213]]\n",
      "[[-0.02081376]\n",
      " [ 0.06871025]]\n",
      "[[-0.02081192]\n",
      " [ 0.06869838]]\n",
      "[[-0.02081008]\n",
      " [ 0.06868651]]\n",
      "[[-0.02080824]\n",
      " [ 0.06867466]]\n",
      "[[-0.02080639]\n",
      " [ 0.06866282]]\n",
      "[[-0.02080454]\n",
      " [ 0.06865098]]\n",
      "[[-0.02080269]\n",
      " [ 0.06863916]]\n",
      "[[-0.02080083]\n",
      " [ 0.06862734]]\n",
      "[[-0.02079896]\n",
      " [ 0.06861553]]\n",
      "[[-0.0207971 ]\n",
      " [ 0.06860373]]\n",
      "[[-0.02079522]\n",
      " [ 0.06859194]]\n",
      "[[-0.02079335]\n",
      " [ 0.06858016]]\n",
      "[[-0.02079147]\n",
      " [ 0.06856839]]\n",
      "[[-0.02078958]\n",
      " [ 0.06855663]]\n",
      "[[-0.02078769]\n",
      " [ 0.06854487]]\n",
      "[[-0.0207858 ]\n",
      " [ 0.06853313]]\n",
      "[[-0.02078391]\n",
      " [ 0.06852139]]\n",
      "[[-0.02078201]\n",
      " [ 0.06850966]]\n",
      "[[-0.0207801 ]\n",
      " [ 0.06849795]]\n",
      "[[-0.02077819]\n",
      " [ 0.06848624]]\n",
      "[[-0.02077628]\n",
      " [ 0.06847454]]\n",
      "[[-0.02077437]\n",
      " [ 0.06846284]]\n",
      "[[-0.02077245]\n",
      " [ 0.06845116]]\n",
      "[[-0.02077052]\n",
      " [ 0.06843949]]\n",
      "[[-0.02076859]\n",
      " [ 0.06842782]]\n",
      "[[-0.02076666]\n",
      " [ 0.06841616]]\n",
      "[[-0.02076473]\n",
      " [ 0.06840451]]\n",
      "[[-0.02076279]\n",
      " [ 0.06839288]]\n",
      "[[-0.02076085]\n",
      " [ 0.06838124]]\n",
      "[[-0.0207589 ]\n",
      " [ 0.06836962]]\n",
      "[[-0.02075695]\n",
      " [ 0.06835801]]\n",
      "[[-0.02075499]\n",
      " [ 0.0683464 ]]\n",
      "[[-0.02075304]\n",
      " [ 0.06833481]]\n",
      "[[-0.02075107]\n",
      " [ 0.06832322]]\n",
      "[[-0.02074911]\n",
      " [ 0.06831164]]\n",
      "[[-0.02074714]\n",
      " [ 0.06830007]]\n",
      "[[-0.02074517]\n",
      " [ 0.06828851]]\n",
      "[[-0.02074319]\n",
      " [ 0.06827695]]\n",
      "[[-0.02074121]\n",
      " [ 0.06826541]]\n",
      "[[-0.02073922]\n",
      " [ 0.06825387]]\n",
      "[[-0.02073724]\n",
      " [ 0.06824234]]\n",
      "[[-0.02073524]\n",
      " [ 0.06823082]]\n",
      "[[-0.02073325]\n",
      " [ 0.06821931]]\n",
      "[[-0.02073125]\n",
      " [ 0.06820781]]\n",
      "[[-0.02072925]\n",
      " [ 0.06819631]]\n",
      "[[-0.02072724]\n",
      " [ 0.06818482]]\n",
      "[[-0.02072523]\n",
      " [ 0.06817334]]\n",
      "[[-0.02072322]\n",
      " [ 0.06816187]]\n",
      "[[-0.0207212 ]\n",
      " [ 0.06815041]]\n",
      "[[-0.02071918]\n",
      " [ 0.06813896]]\n",
      "[[-0.02071716]\n",
      " [ 0.06812751]]\n",
      "[[-0.02071513]\n",
      " [ 0.06811608]]\n",
      "[[-0.0207131 ]\n",
      " [ 0.06810465]]\n",
      "[[-0.02071106]\n",
      " [ 0.06809323]]\n",
      "[[-0.02070903]\n",
      " [ 0.06808182]]\n",
      "[[-0.02070698]\n",
      " [ 0.06807041]]\n",
      "[[-0.02070494]\n",
      " [ 0.06805902]]\n",
      "[[-0.02070289]\n",
      " [ 0.06804763]]\n",
      "[[-0.02070084]\n",
      " [ 0.06803625]]\n",
      "[[-0.02069878]\n",
      " [ 0.06802488]]\n",
      "[[-0.02069673]\n",
      " [ 0.06801351]]\n",
      "[[-0.02069466]\n",
      " [ 0.06800216]]\n",
      "[[-0.0206926 ]\n",
      " [ 0.06799081]]\n",
      "[[-0.02069053]\n",
      " [ 0.06797947]]\n",
      "[[-0.02068846]\n",
      " [ 0.06796814]]\n",
      "[[-0.02068638]\n",
      " [ 0.06795681]]\n",
      "[[-0.0206843]\n",
      " [ 0.0679455]]\n",
      "[[-0.02068222]\n",
      " [ 0.06793419]]\n",
      "[[-0.02068014]\n",
      " [ 0.06792289]]\n",
      "[[-0.02067805]\n",
      " [ 0.0679116 ]]\n",
      "[[-0.02067595]\n",
      " [ 0.06790032]]\n",
      "[[-0.02067386]\n",
      " [ 0.06788904]]\n",
      "[[-0.02067176]\n",
      " [ 0.06787777]]\n",
      "[[-0.02066966]\n",
      " [ 0.06786651]]\n",
      "[[-0.02066755]\n",
      " [ 0.06785526]]\n",
      "[[-0.02066545]\n",
      " [ 0.06784401]]\n",
      "Cost after iteration 1700: 0.477875\n",
      "[[-0.02066333]\n",
      " [ 0.06783278]]\n",
      "[[-0.02066122]\n",
      " [ 0.06782155]]\n",
      "[[-0.0206591 ]\n",
      " [ 0.06781033]]\n",
      "[[-0.02065698]\n",
      " [ 0.06779911]]\n",
      "[[-0.02065486]\n",
      " [ 0.06778791]]\n",
      "[[-0.02065273]\n",
      " [ 0.06777671]]\n",
      "[[-0.0206506 ]\n",
      " [ 0.06776552]]\n",
      "[[-0.02064846]\n",
      " [ 0.06775434]]\n",
      "[[-0.02064633]\n",
      " [ 0.06774316]]\n",
      "[[-0.02064419]\n",
      " [ 0.06773199]]\n",
      "[[-0.02064204]\n",
      " [ 0.06772083]]\n",
      "[[-0.0206399 ]\n",
      " [ 0.06770968]]\n",
      "[[-0.02063775]\n",
      " [ 0.06769854]]\n",
      "[[-0.02063559]\n",
      " [ 0.0676874 ]]\n",
      "[[-0.02063344]\n",
      " [ 0.06767627]]\n",
      "[[-0.02063128]\n",
      " [ 0.06766515]]\n",
      "[[-0.02062912]\n",
      " [ 0.06765404]]\n",
      "[[-0.02062695]\n",
      " [ 0.06764293]]\n",
      "[[-0.02062479]\n",
      " [ 0.06763183]]\n",
      "[[-0.02062262]\n",
      " [ 0.06762074]]\n",
      "[[-0.02062044]\n",
      " [ 0.06760965]]\n",
      "[[-0.02061827]\n",
      " [ 0.06759858]]\n",
      "[[-0.02061609]\n",
      " [ 0.06758751]]\n",
      "[[-0.0206139 ]\n",
      " [ 0.06757645]]\n",
      "[[-0.02061172]\n",
      " [ 0.06756539]]\n",
      "[[-0.02060953]\n",
      " [ 0.06755434]]\n",
      "[[-0.02060734]\n",
      " [ 0.0675433 ]]\n",
      "[[-0.02060514]\n",
      " [ 0.06753227]]\n",
      "[[-0.02060295]\n",
      " [ 0.06752125]]\n",
      "[[-0.02060075]\n",
      " [ 0.06751023]]\n",
      "[[-0.02059854]\n",
      " [ 0.06749922]]\n",
      "[[-0.02059634]\n",
      " [ 0.06748822]]\n",
      "[[-0.02059413]\n",
      " [ 0.06747722]]\n",
      "[[-0.02059192]\n",
      " [ 0.06746623]]\n",
      "[[-0.0205897 ]\n",
      " [ 0.06745525]]\n",
      "[[-0.02058748]\n",
      " [ 0.06744428]]\n",
      "[[-0.02058526]\n",
      " [ 0.06743331]]\n",
      "[[-0.02058304]\n",
      " [ 0.06742235]]\n",
      "[[-0.02058082]\n",
      " [ 0.0674114 ]]\n",
      "[[-0.02057859]\n",
      " [ 0.06740045]]\n",
      "[[-0.02057636]\n",
      " [ 0.06738951]]\n",
      "[[-0.02057412]\n",
      " [ 0.06737858]]\n",
      "[[-0.02057188]\n",
      " [ 0.06736766]]\n",
      "[[-0.02056964]\n",
      " [ 0.06735674]]\n",
      "[[-0.0205674 ]\n",
      " [ 0.06734583]]\n",
      "[[-0.02056516]\n",
      " [ 0.06733493]]\n",
      "[[-0.02056291]\n",
      " [ 0.06732404]]\n",
      "[[-0.02056066]\n",
      " [ 0.06731315]]\n",
      "[[-0.02055841]\n",
      " [ 0.06730227]]\n",
      "[[-0.02055615]\n",
      " [ 0.06729139]]\n",
      "[[-0.02055389]\n",
      " [ 0.06728052]]\n",
      "[[-0.02055163]\n",
      " [ 0.06726966]]\n",
      "[[-0.02054937]\n",
      " [ 0.06725881]]\n",
      "[[-0.0205471 ]\n",
      " [ 0.06724796]]\n",
      "[[-0.02054483]\n",
      " [ 0.06723713]]\n",
      "[[-0.02054256]\n",
      " [ 0.06722629]]\n",
      "[[-0.02054028]\n",
      " [ 0.06721547]]\n",
      "[[-0.02053801]\n",
      " [ 0.06720465]]\n",
      "[[-0.02053573]\n",
      " [ 0.06719384]]\n",
      "[[-0.02053344]\n",
      " [ 0.06718303]]\n",
      "[[-0.02053116]\n",
      " [ 0.06717224]]\n",
      "[[-0.02052887]\n",
      " [ 0.06716144]]\n",
      "[[-0.02052658]\n",
      " [ 0.06715066]]\n",
      "[[-0.02052429]\n",
      " [ 0.06713988]]\n",
      "[[-0.02052199]\n",
      " [ 0.06712911]]\n",
      "[[-0.0205197 ]\n",
      " [ 0.06711835]]\n",
      "[[-0.0205174 ]\n",
      " [ 0.06710759]]\n",
      "[[-0.02051509]\n",
      " [ 0.06709684]]\n",
      "[[-0.02051279]\n",
      " [ 0.0670861 ]]\n",
      "[[-0.02051048]\n",
      " [ 0.06707536]]\n",
      "[[-0.02050817]\n",
      " [ 0.06706463]]\n",
      "[[-0.02050586]\n",
      " [ 0.06705391]]\n",
      "[[-0.02050354]\n",
      " [ 0.06704319]]\n",
      "[[-0.02050122]\n",
      " [ 0.06703248]]\n",
      "[[-0.0204989 ]\n",
      " [ 0.06702178]]\n",
      "[[-0.02049658]\n",
      " [ 0.06701108]]\n",
      "[[-0.02049426]\n",
      " [ 0.06700039]]\n",
      "[[-0.02049193]\n",
      " [ 0.06698971]]\n",
      "[[-0.0204896 ]\n",
      " [ 0.06697903]]\n",
      "[[-0.02048727]\n",
      " [ 0.06696836]]\n",
      "[[-0.02048493]\n",
      " [ 0.0669577 ]]\n",
      "[[-0.02048259]\n",
      " [ 0.06694704]]\n",
      "[[-0.02048025]\n",
      " [ 0.06693639]]\n",
      "[[-0.02047791]\n",
      " [ 0.06692575]]\n",
      "[[-0.02047557]\n",
      " [ 0.06691511]]\n",
      "[[-0.02047322]\n",
      " [ 0.06690448]]\n",
      "[[-0.02047087]\n",
      " [ 0.06689386]]\n",
      "[[-0.02046852]\n",
      " [ 0.06688324]]\n",
      "[[-0.02046617]\n",
      " [ 0.06687263]]\n",
      "[[-0.02046381]\n",
      " [ 0.06686202]]\n",
      "[[-0.02046145]\n",
      " [ 0.06685142]]\n",
      "[[-0.02045909]\n",
      " [ 0.06684083]]\n",
      "[[-0.02045673]\n",
      " [ 0.06683025]]\n",
      "[[-0.02045437]\n",
      " [ 0.06681967]]\n",
      "[[-0.020452 ]\n",
      " [ 0.0668091]]\n",
      "[[-0.02044963]\n",
      " [ 0.06679853]]\n",
      "[[-0.02044726]\n",
      " [ 0.06678797]]\n",
      "[[-0.02044488]\n",
      " [ 0.06677742]]\n",
      "[[-0.02044251]\n",
      " [ 0.06676687]]\n",
      "[[-0.02044013]\n",
      " [ 0.06675633]]\n",
      "Cost after iteration 1800: 0.469295\n",
      "[[-0.02043775]\n",
      " [ 0.06674579]]\n",
      "[[-0.02043537]\n",
      " [ 0.06673526]]\n",
      "[[-0.02043298]\n",
      " [ 0.06672474]]\n",
      "[[-0.02043059]\n",
      " [ 0.06671423]]\n",
      "[[-0.0204282 ]\n",
      " [ 0.06670372]]\n",
      "[[-0.02042581]\n",
      " [ 0.06669321]]\n",
      "[[-0.02042342]\n",
      " [ 0.06668272]]\n",
      "[[-0.02042102]\n",
      " [ 0.06667223]]\n",
      "[[-0.02041862]\n",
      " [ 0.06666174]]\n",
      "[[-0.02041622]\n",
      " [ 0.06665127]]\n",
      "[[-0.02041382]\n",
      " [ 0.06664079]]\n",
      "[[-0.02041142]\n",
      " [ 0.06663033]]\n",
      "[[-0.02040901]\n",
      " [ 0.06661987]]\n",
      "[[-0.0204066 ]\n",
      " [ 0.06660941]]\n",
      "[[-0.02040419]\n",
      " [ 0.06659897]]\n",
      "[[-0.02040178]\n",
      " [ 0.06658853]]\n",
      "[[-0.02039936]\n",
      " [ 0.06657809]]\n",
      "[[-0.02039694]\n",
      " [ 0.06656766]]\n",
      "[[-0.02039452]\n",
      " [ 0.06655724]]\n",
      "[[-0.0203921 ]\n",
      " [ 0.06654682]]\n",
      "[[-0.02038968]\n",
      " [ 0.06653641]]\n",
      "[[-0.02038725]\n",
      " [ 0.06652601]]\n",
      "[[-0.02038483]\n",
      " [ 0.06651561]]\n",
      "[[-0.0203824 ]\n",
      " [ 0.06650522]]\n",
      "[[-0.02037997]\n",
      " [ 0.06649483]]\n",
      "[[-0.02037753]\n",
      " [ 0.06648445]]\n",
      "[[-0.0203751 ]\n",
      " [ 0.06647407]]\n",
      "[[-0.02037266]\n",
      " [ 0.06646371]]\n",
      "[[-0.02037022]\n",
      " [ 0.06645334]]\n",
      "[[-0.02036778]\n",
      " [ 0.06644299]]\n",
      "[[-0.02036533]\n",
      " [ 0.06643264]]\n",
      "[[-0.02036289]\n",
      " [ 0.06642229]]\n",
      "[[-0.02036044]\n",
      " [ 0.06641195]]\n",
      "[[-0.02035799]\n",
      " [ 0.06640162]]\n",
      "[[-0.02035554]\n",
      " [ 0.06639129]]\n",
      "[[-0.02035309]\n",
      " [ 0.06638097]]\n",
      "[[-0.02035063]\n",
      " [ 0.06637066]]\n",
      "[[-0.02034818]\n",
      " [ 0.06636035]]\n",
      "[[-0.02034572]\n",
      " [ 0.06635004]]\n",
      "[[-0.02034326]\n",
      " [ 0.06633974]]\n",
      "[[-0.02034079]\n",
      " [ 0.06632945]]\n",
      "[[-0.02033833]\n",
      " [ 0.06631917]]\n",
      "[[-0.02033586]\n",
      " [ 0.06630889]]\n",
      "[[-0.02033339]\n",
      " [ 0.06629861]]\n",
      "[[-0.02033092]\n",
      " [ 0.06628834]]\n",
      "[[-0.02032845]\n",
      " [ 0.06627808]]\n",
      "[[-0.02032598]\n",
      " [ 0.06626782]]\n",
      "[[-0.0203235 ]\n",
      " [ 0.06625757]]\n",
      "[[-0.02032102]\n",
      " [ 0.06624733]]\n",
      "[[-0.02031854]\n",
      " [ 0.06623709]]\n",
      "[[-0.02031606]\n",
      " [ 0.06622685]]\n",
      "[[-0.02031358]\n",
      " [ 0.06621662]]\n",
      "[[-0.02031109]\n",
      " [ 0.0662064 ]]\n",
      "[[-0.02030861]\n",
      " [ 0.06619618]]\n",
      "[[-0.02030612]\n",
      " [ 0.06618597]]\n",
      "[[-0.02030363]\n",
      " [ 0.06617577]]\n",
      "[[-0.02030113]\n",
      " [ 0.06616557]]\n",
      "[[-0.02029864]\n",
      " [ 0.06615537]]\n",
      "[[-0.02029614]\n",
      " [ 0.06614518]]\n",
      "[[-0.02029365]\n",
      " [ 0.066135  ]]\n",
      "[[-0.02029115]\n",
      " [ 0.06612482]]\n",
      "[[-0.02028865]\n",
      " [ 0.06611465]]\n",
      "[[-0.02028614]\n",
      " [ 0.06610448]]\n",
      "[[-0.02028364]\n",
      " [ 0.06609432]]\n",
      "[[-0.02028113]\n",
      " [ 0.06608417]]\n",
      "[[-0.02027863]\n",
      " [ 0.06607402]]\n",
      "[[-0.02027612]\n",
      " [ 0.06606387]]\n",
      "[[-0.0202736 ]\n",
      " [ 0.06605373]]\n",
      "[[-0.02027109]\n",
      " [ 0.0660436 ]]\n",
      "[[-0.02026858]\n",
      " [ 0.06603347]]\n",
      "[[-0.02026606]\n",
      " [ 0.06602335]]\n",
      "[[-0.02026354]\n",
      " [ 0.06601323]]\n",
      "[[-0.02026102]\n",
      " [ 0.06600312]]\n",
      "[[-0.0202585 ]\n",
      " [ 0.06599301]]\n",
      "[[-0.02025598]\n",
      " [ 0.06598291]]\n",
      "[[-0.02025345]\n",
      " [ 0.06597282]]\n",
      "[[-0.02025093]\n",
      " [ 0.06596273]]\n",
      "[[-0.0202484 ]\n",
      " [ 0.06595265]]\n",
      "[[-0.02024587]\n",
      " [ 0.06594257]]\n",
      "[[-0.02024334]\n",
      " [ 0.06593249]]\n",
      "[[-0.02024081]\n",
      " [ 0.06592242]]\n",
      "[[-0.02023827]\n",
      " [ 0.06591236]]\n",
      "[[-0.02023573]\n",
      " [ 0.0659023 ]]\n",
      "[[-0.0202332 ]\n",
      " [ 0.06589225]]\n",
      "[[-0.02023066]\n",
      " [ 0.06588221]]\n",
      "[[-0.02022812]\n",
      " [ 0.06587216]]\n",
      "[[-0.02022557]\n",
      " [ 0.06586213]]\n",
      "[[-0.02022303]\n",
      " [ 0.0658521 ]]\n",
      "[[-0.02022048]\n",
      " [ 0.06584207]]\n",
      "[[-0.02021794]\n",
      " [ 0.06583205]]\n",
      "[[-0.02021539]\n",
      " [ 0.06582204]]\n",
      "[[-0.02021284]\n",
      " [ 0.06581203]]\n",
      "[[-0.02021029]\n",
      " [ 0.06580202]]\n",
      "[[-0.02020773]\n",
      " [ 0.06579202]]\n",
      "[[-0.02020518]\n",
      " [ 0.06578203]]\n",
      "[[-0.02020262]\n",
      " [ 0.06577204]]\n",
      "[[-0.02020007]\n",
      " [ 0.06576206]]\n",
      "[[-0.02019751]\n",
      " [ 0.06575208]]\n",
      "[[-0.02019495]\n",
      " [ 0.06574211]]\n",
      "[[-0.02019238]\n",
      " [ 0.06573214]]\n",
      "Cost after iteration 1900: 0.460949\n",
      "[[-0.02018982]\n",
      " [ 0.06572218]]\n",
      "[[-0.02018725]\n",
      " [ 0.06571222]]\n",
      "[[-0.02018469]\n",
      " [ 0.06570227]]\n",
      "[[-0.02018212]\n",
      " [ 0.06569232]]\n",
      "[[-0.02017955]\n",
      " [ 0.06568238]]\n",
      "[[-0.02017698]\n",
      " [ 0.06567244]]\n",
      "[[-0.02017441]\n",
      " [ 0.06566251]]\n",
      "[[-0.02017183]\n",
      " [ 0.06565258]]\n",
      "[[-0.02016926]\n",
      " [ 0.06564266]]\n",
      "[[-0.02016668]\n",
      " [ 0.06563275]]\n",
      "[[-0.0201641 ]\n",
      " [ 0.06562283]]\n",
      "[[-0.02016152]\n",
      " [ 0.06561293]]\n",
      "[[-0.02015894]\n",
      " [ 0.06560303]]\n",
      "[[-0.02015636]\n",
      " [ 0.06559313]]\n",
      "[[-0.02015377]\n",
      " [ 0.06558324]]\n",
      "[[-0.02015119]\n",
      " [ 0.06557335]]\n",
      "[[-0.0201486 ]\n",
      " [ 0.06556347]]\n",
      "[[-0.02014601]\n",
      " [ 0.0655536 ]]\n",
      "[[-0.02014342]\n",
      " [ 0.06554372]]\n",
      "[[-0.02014083]\n",
      " [ 0.06553386]]\n",
      "[[-0.02013824]\n",
      " [ 0.065524  ]]\n",
      "[[-0.02013565]\n",
      " [ 0.06551414]]\n",
      "[[-0.02013305]\n",
      " [ 0.06550429]]\n",
      "[[-0.02013046]\n",
      " [ 0.06549444]]\n",
      "[[-0.02012786]\n",
      " [ 0.0654846 ]]\n",
      "[[-0.02012526]\n",
      " [ 0.06547477]]\n",
      "[[-0.02012266]\n",
      " [ 0.06546493]]\n",
      "[[-0.02012006]\n",
      " [ 0.06545511]]\n",
      "[[-0.02011746]\n",
      " [ 0.06544529]]\n",
      "[[-0.02011485]\n",
      " [ 0.06543547]]\n",
      "[[-0.02011225]\n",
      " [ 0.06542566]]\n",
      "[[-0.02010964]\n",
      " [ 0.06541585]]\n",
      "[[-0.02010703]\n",
      " [ 0.06540605]]\n",
      "[[-0.02010442]\n",
      " [ 0.06539625]]\n",
      "[[-0.02010181]\n",
      " [ 0.06538646]]\n",
      "[[-0.0200992 ]\n",
      " [ 0.06537667]]\n",
      "[[-0.02009659]\n",
      " [ 0.06536689]]\n",
      "[[-0.02009397]\n",
      " [ 0.06535711]]\n",
      "[[-0.02009136]\n",
      " [ 0.06534734]]\n",
      "[[-0.02008874]\n",
      " [ 0.06533757]]\n",
      "[[-0.02008612]\n",
      " [ 0.06532781]]\n",
      "[[-0.0200835 ]\n",
      " [ 0.06531805]]\n",
      "[[-0.02008088]\n",
      " [ 0.06530829]]\n",
      "[[-0.02007826]\n",
      " [ 0.06529854]]\n",
      "[[-0.02007564]\n",
      " [ 0.0652888 ]]\n",
      "[[-0.02007301]\n",
      " [ 0.06527906]]\n",
      "[[-0.02007039]\n",
      " [ 0.06526932]]\n",
      "[[-0.02006776]\n",
      " [ 0.06525959]]\n",
      "[[-0.02006513]\n",
      " [ 0.06524987]]\n",
      "[[-0.0200625 ]\n",
      " [ 0.06524015]]\n",
      "[[-0.02005987]\n",
      " [ 0.06523043]]\n",
      "[[-0.02005724]\n",
      " [ 0.06522072]]\n",
      "[[-0.02005461]\n",
      " [ 0.06521101]]\n",
      "[[-0.02005197]\n",
      " [ 0.06520131]]\n",
      "[[-0.02004934]\n",
      " [ 0.06519161]]\n",
      "[[-0.0200467 ]\n",
      " [ 0.06518192]]\n",
      "[[-0.02004407]\n",
      " [ 0.06517223]]\n",
      "[[-0.02004143]\n",
      " [ 0.06516255]]\n",
      "[[-0.02003879]\n",
      " [ 0.06515287]]\n",
      "[[-0.02003615]\n",
      " [ 0.0651432 ]]\n",
      "[[-0.0200335 ]\n",
      " [ 0.06513353]]\n",
      "[[-0.02003086]\n",
      " [ 0.06512386]]\n",
      "[[-0.02002822]\n",
      " [ 0.0651142 ]]\n",
      "[[-0.02002557]\n",
      " [ 0.06510455]]\n",
      "[[-0.02002293]\n",
      " [ 0.0650949 ]]\n",
      "[[-0.02002028]\n",
      " [ 0.06508525]]\n",
      "[[-0.02001763]\n",
      " [ 0.06507561]]\n",
      "[[-0.02001498]\n",
      " [ 0.06506597]]\n",
      "[[-0.02001233]\n",
      " [ 0.06505634]]\n",
      "[[-0.02000968]\n",
      " [ 0.06504671]]\n",
      "[[-0.02000702]\n",
      " [ 0.06503708]]\n",
      "[[-0.02000437]\n",
      " [ 0.06502747]]\n",
      "[[-0.02000171]\n",
      " [ 0.06501785]]\n",
      "[[-0.01999906]\n",
      " [ 0.06500824]]\n",
      "[[-0.0199964 ]\n",
      " [ 0.06499863]]\n",
      "[[-0.01999374]\n",
      " [ 0.06498903]]\n",
      "[[-0.01999108]\n",
      " [ 0.06497944]]\n",
      "[[-0.01998842]\n",
      " [ 0.06496984]]\n",
      "[[-0.01998576]\n",
      " [ 0.06496026]]\n",
      "[[-0.0199831 ]\n",
      " [ 0.06495067]]\n",
      "[[-0.01998043]\n",
      " [ 0.06494109]]\n",
      "[[-0.01997777]\n",
      " [ 0.06493152]]\n",
      "[[-0.0199751 ]\n",
      " [ 0.06492195]]\n",
      "[[-0.01997244]\n",
      " [ 0.06491238]]\n",
      "[[-0.01996977]\n",
      " [ 0.06490282]]\n",
      "[[-0.0199671 ]\n",
      " [ 0.06489327]]\n",
      "[[-0.01996443]\n",
      " [ 0.06488371]]\n",
      "[[-0.01996176]\n",
      " [ 0.06487416]]\n",
      "[[-0.01995909]\n",
      " [ 0.06486462]]\n",
      "[[-0.01995642]\n",
      " [ 0.06485508]]\n",
      "[[-0.01995374]\n",
      " [ 0.06484555]]\n",
      "[[-0.01995107]\n",
      " [ 0.06483602]]\n",
      "[[-0.01994839]\n",
      " [ 0.06482649]]\n",
      "[[-0.01994572]\n",
      " [ 0.06481697]]\n",
      "[[-0.01994304]\n",
      " [ 0.06480745]]\n",
      "[[-0.01994036]\n",
      " [ 0.06479794]]\n",
      "[[-0.01993768]\n",
      " [ 0.06478843]]\n",
      "[[-0.019935  ]\n",
      " [ 0.06477892]]\n",
      "[[-0.01993232]\n",
      " [ 0.06476942]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-db80af875783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-ba66a839ecc0>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Predict test/train set examples (≈ 2 lines of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mY_prediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#Y_prediction_train = predict(w, b, X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-8adb0f6a83ed>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(w, b, X)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mY_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Compute vector \"A\" predicting the probabilities of a cat being present in the picture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0 **  </td> \n",
    "        <td> 0.693147 </td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td> **Train Accuracy**  </td> \n",
    "        <td> 99.04306220095694 % </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>**Test Accuracy** </td> \n",
    "        <td> 70.0 % </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!\n",
    "\n",
    "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-c6c0e9212ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"y = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", you predicted that it is a \\\"\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Y_prediction_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m\"\\\" picture.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52dad672b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Further analysis (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Change your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "my_image = \"my_image.jpg\"   # change this to the name of your image file \n",
    "## END CODE HERE ##\n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "image = image/255.\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What to remember from this assignment:**\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
